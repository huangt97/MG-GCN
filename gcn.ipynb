{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1+cpu\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseDropout(nn.Module):\n",
    "    def __init__(self, dprob=0.5):\n",
    "        super(SparseDropout, self).__init__()\n",
    "        # dprob is ratio of dropout\n",
    "        # convert to keep probability\n",
    "        self.kprob = 1 - dprob\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = ((torch.rand(x._values().size()) + (self.kprob)).floor()).type(torch.uint8)\n",
    "        rc = x._indices()[:, mask]\n",
    "        val = x._values()[mask] * (1.0 / self.kprob)\n",
    "        return torch.sparse.FloatTensor(rc, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.sparse_dropout = SparseDropout(dropout)\n",
    "        self.dropout = dropout\n",
    "        self.out1_names = [param[0] for param in self.gc1.named_parameters()]\n",
    "        self.out2_names = [param[0] for param in self.gc2.named_parameters()]\n",
    "\n",
    "    def forward(self, x, adj, dropout=False):\n",
    "        if dropout:\n",
    "            adj = self.sparse_dropout(adj)\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def hidden(self, x, adj):\n",
    "        out1 = self.gc1(x, adj)\n",
    "        x = F.relu(out1)\n",
    "        x = F.dropout(x, self.dropout, training=False)\n",
    "        out2 = self.gc2(x, adj)\n",
    "        return out1, out2\n",
    "\n",
    "    def param_names(self):\n",
    "        return self.out1_names, self.out2_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features = (1402220, 12) type = <class 'numpy.ndarray'>\n",
      "train_index 572\n",
      "val_index 82\n",
      "test_index 162\n",
      "adj torch.Size([1402220, 1402220])\n",
      "features torch.Size([1402220, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\sparse\\_index.py:127: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "sparse_adj, sparse_adj_train,sparse_adj_train_all,features, train_feature,train_feature_all,labels, train_labels, id_train, id_valid, id_test, num_labels = Origin_load_ether_data(cuda=False)\n",
    "sparse_adj.setdiag(1.0)\n",
    "sparse_adj_train.setdiag(1.0)\n",
    "sparse_adj_train_all.setdiag(1.0)\n",
    "\n",
    "coo = sparse_adj.tocoo()\n",
    "values = coo.data\n",
    "indices = np.vstack((coo.row, coo.col)).astype(np.int32)\n",
    "_i = torch.LongTensor(indices)\n",
    "_v = torch.FloatTensor(values)\n",
    "shape = coo.shape\n",
    "adj = torch.sparse.FloatTensor(_i, _v, torch.Size(shape))\n",
    "# dataset splits\n",
    "idx_train = np.array(np.zeros(features.shape[0]), dtype=np.bool)\n",
    "idx_val = np.array(np.zeros(features.shape[0]), dtype=np.bool)\n",
    "idx_test = np.array(np.zeros(features.shape[0]), dtype=np.bool)\n",
    "id_train = np.array(id_train)\n",
    "id_valid = np.array(id_valid)\n",
    "id_test = np.array(id_test)\n",
    "idx_train[id_train] = True\n",
    "idx_val[id_valid] = True\n",
    "idx_test[id_test] = True\n",
    "idx_train = torch.tensor(idx_train)\n",
    "idx_val = torch.tensor(idx_val)\n",
    "idx_test = torch.tensor(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 5375.3721 acc_train: 0.0734 loss_val: 4146.2910 acc_val: 0.0976 time: 2.6852s\n",
      "Epoch: 0002 loss_train: 4019.2634 acc_train: 0.0857 loss_val: 3081.1636 acc_val: 0.1098 time: 2.9342s\n",
      "Epoch: 0003 loss_train: 2890.0652 acc_train: 0.0997 loss_val: 2339.6265 acc_val: 0.1463 time: 2.8572s\n",
      "Epoch: 0004 loss_train: 1968.0265 acc_train: 0.1294 loss_val: 1781.5703 acc_val: 0.1463 time: 2.6572s\n",
      "Epoch: 0005 loss_train: 1224.3937 acc_train: 0.1626 loss_val: 1356.0747 acc_val: 0.1463 time: 2.7452s\n",
      "Epoch: 0006 loss_train: 687.6473 acc_train: 0.2063 loss_val: 1212.9945 acc_val: 0.1463 time: 2.6842s\n",
      "Epoch: 0007 loss_train: 615.3702 acc_train: 0.2115 loss_val: 1125.9908 acc_val: 0.1707 time: 2.7011s\n",
      "Epoch: 0008 loss_train: 678.8439 acc_train: 0.2185 loss_val: 1075.7802 acc_val: 0.1707 time: 2.9672s\n",
      "Epoch: 0009 loss_train: 725.3268 acc_train: 0.2150 loss_val: 1052.7775 acc_val: 0.2683 time: 3.0352s\n",
      "Epoch: 0010 loss_train: 744.7923 acc_train: 0.2815 loss_val: 1101.5941 acc_val: 0.2927 time: 2.9547s\n",
      "Epoch: 0011 loss_train: 733.9261 acc_train: 0.2832 loss_val: 1135.0555 acc_val: 0.2073 time: 2.7612s\n",
      "Epoch: 0012 loss_train: 704.5038 acc_train: 0.2273 loss_val: 1147.0917 acc_val: 0.2073 time: 2.5882s\n",
      "Epoch: 0013 loss_train: 662.3008 acc_train: 0.2378 loss_val: 1120.9625 acc_val: 0.2073 time: 2.6682s\n",
      "Epoch: 0014 loss_train: 607.6118 acc_train: 0.2360 loss_val: 1063.5580 acc_val: 0.2317 time: 2.6152s\n",
      "Epoch: 0015 loss_train: 560.4529 acc_train: 0.2483 loss_val: 964.0978 acc_val: 0.2195 time: 2.6682s\n",
      "Epoch: 0016 loss_train: 511.2546 acc_train: 0.2587 loss_val: 848.1710 acc_val: 0.2073 time: 2.6822s\n",
      "Epoch: 0017 loss_train: 457.0031 acc_train: 0.2657 loss_val: 734.6357 acc_val: 0.2317 time: 2.5502s\n",
      "Epoch: 0018 loss_train: 401.5895 acc_train: 0.2692 loss_val: 682.4604 acc_val: 0.2317 time: 2.5522s\n",
      "Epoch: 0019 loss_train: 350.3322 acc_train: 0.2710 loss_val: 645.0557 acc_val: 0.2561 time: 2.7972s\n",
      "Epoch: 0020 loss_train: 317.6206 acc_train: 0.2797 loss_val: 617.2216 acc_val: 0.3171 time: 2.5902s\n",
      "Epoch: 0021 loss_train: 274.5909 acc_train: 0.3077 loss_val: 601.0444 acc_val: 0.3171 time: 2.5282s\n",
      "Epoch: 0022 loss_train: 237.2107 acc_train: 0.3322 loss_val: 598.4760 acc_val: 0.4024 time: 2.5822s\n",
      "Epoch: 0023 loss_train: 207.2630 acc_train: 0.3864 loss_val: 604.3688 acc_val: 0.4390 time: 2.6162s\n",
      "Epoch: 0024 loss_train: 181.6819 acc_train: 0.4021 loss_val: 611.8644 acc_val: 0.4512 time: 2.9662s\n",
      "Epoch: 0025 loss_train: 160.2854 acc_train: 0.3934 loss_val: 621.2719 acc_val: 0.3902 time: 2.4712s\n",
      "Epoch: 0026 loss_train: 142.2083 acc_train: 0.4126 loss_val: 634.1669 acc_val: 0.3659 time: 2.7962s\n",
      "Epoch: 0027 loss_train: 126.4521 acc_train: 0.4231 loss_val: 647.8849 acc_val: 0.3537 time: 2.7152s\n",
      "Epoch: 0028 loss_train: 112.2127 acc_train: 0.4056 loss_val: 658.5211 acc_val: 0.3537 time: 2.5782s\n",
      "Epoch: 0029 loss_train: 98.8454 acc_train: 0.4091 loss_val: 673.7395 acc_val: 0.3171 time: 2.5612s\n",
      "Epoch: 0030 loss_train: 89.3436 acc_train: 0.3969 loss_val: 696.8768 acc_val: 0.3171 time: 2.7952s\n",
      "Epoch: 0031 loss_train: 85.2749 acc_train: 0.3829 loss_val: 715.4868 acc_val: 0.3293 time: 2.7222s\n",
      "Epoch: 0032 loss_train: 90.2789 acc_train: 0.3706 loss_val: 724.4833 acc_val: 0.3415 time: 2.6362s\n",
      "Epoch: 0033 loss_train: 103.4732 acc_train: 0.3706 loss_val: 718.9113 acc_val: 0.3659 time: 2.4942s\n",
      "Epoch: 0034 loss_train: 109.0804 acc_train: 0.3776 loss_val: 699.8644 acc_val: 0.3659 time: 2.7032s\n",
      "Epoch: 0035 loss_train: 101.1681 acc_train: 0.3829 loss_val: 669.6804 acc_val: 0.3537 time: 2.7202s\n",
      "Epoch: 0036 loss_train: 83.0503 acc_train: 0.3759 loss_val: 631.2905 acc_val: 0.3780 time: 2.4612s\n",
      "Epoch: 0037 loss_train: 66.8064 acc_train: 0.4003 loss_val: 593.2028 acc_val: 0.3659 time: 2.6442s\n",
      "Epoch: 0038 loss_train: 60.1024 acc_train: 0.4091 loss_val: 558.2629 acc_val: 0.3659 time: 2.7272s\n",
      "Epoch: 0039 loss_train: 58.9025 acc_train: 0.4196 loss_val: 532.1567 acc_val: 0.3780 time: 2.7202s\n",
      "Epoch: 0040 loss_train: 59.6769 acc_train: 0.4283 loss_val: 511.7380 acc_val: 0.3902 time: 2.4962s\n",
      "Epoch: 0041 loss_train: 60.1839 acc_train: 0.4353 loss_val: 493.4760 acc_val: 0.3902 time: 2.4852s\n",
      "Epoch: 0042 loss_train: 60.3706 acc_train: 0.4371 loss_val: 477.3431 acc_val: 0.4146 time: 2.4762s\n",
      "Epoch: 0043 loss_train: 60.0276 acc_train: 0.4476 loss_val: 463.5501 acc_val: 0.4268 time: 2.4952s\n",
      "Epoch: 0044 loss_train: 59.1469 acc_train: 0.4441 loss_val: 451.5077 acc_val: 0.4146 time: 2.4522s\n",
      "Epoch: 0045 loss_train: 57.7310 acc_train: 0.4406 loss_val: 440.9752 acc_val: 0.4146 time: 2.6492s\n",
      "Epoch: 0046 loss_train: 55.8012 acc_train: 0.4406 loss_val: 431.8111 acc_val: 0.4146 time: 2.6962s\n",
      "Epoch: 0047 loss_train: 53.3669 acc_train: 0.4353 loss_val: 423.9432 acc_val: 0.4146 time: 2.7142s\n",
      "Epoch: 0048 loss_train: 50.3103 acc_train: 0.4423 loss_val: 417.5131 acc_val: 0.4146 time: 2.5362s\n",
      "Epoch: 0049 loss_train: 46.7278 acc_train: 0.4388 loss_val: 412.1102 acc_val: 0.3902 time: 2.5852s\n",
      "Epoch: 0050 loss_train: 42.9081 acc_train: 0.4301 loss_val: 407.5339 acc_val: 0.3780 time: 2.5442s\n",
      "Epoch: 0051 loss_train: 39.2559 acc_train: 0.4353 loss_val: 403.6426 acc_val: 0.3780 time: 2.6542s\n",
      "Epoch: 0052 loss_train: 35.7837 acc_train: 0.4371 loss_val: 400.2664 acc_val: 0.3780 time: 2.7312s\n",
      "Epoch: 0053 loss_train: 33.1380 acc_train: 0.4336 loss_val: 398.0911 acc_val: 0.3659 time: 2.8212s\n",
      "Epoch: 0054 loss_train: 33.6131 acc_train: 0.4266 loss_val: 395.0902 acc_val: 0.3780 time: 2.7252s\n",
      "Epoch: 0055 loss_train: 37.3522 acc_train: 0.4301 loss_val: 385.8367 acc_val: 0.3780 time: 2.5472s\n",
      "Epoch: 0056 loss_train: 37.5027 acc_train: 0.4318 loss_val: 369.0022 acc_val: 0.3659 time: 2.5112s\n",
      "Epoch: 0057 loss_train: 33.8449 acc_train: 0.4388 loss_val: 351.3345 acc_val: 0.3780 time: 2.5162s\n",
      "Epoch: 0058 loss_train: 28.8579 acc_train: 0.4528 loss_val: 335.1808 acc_val: 0.3902 time: 2.4922s\n",
      "Epoch: 0059 loss_train: 27.6540 acc_train: 0.4615 loss_val: 319.8661 acc_val: 0.3902 time: 2.4562s\n",
      "Epoch: 0060 loss_train: 28.8361 acc_train: 0.4703 loss_val: 306.4364 acc_val: 0.3780 time: 2.8252s\n",
      "Epoch: 0061 loss_train: 29.4638 acc_train: 0.4755 loss_val: 294.7020 acc_val: 0.3902 time: 2.6572s\n",
      "Epoch: 0062 loss_train: 29.5220 acc_train: 0.4878 loss_val: 284.6427 acc_val: 0.3902 time: 2.6732s\n",
      "Epoch: 0063 loss_train: 29.0403 acc_train: 0.4860 loss_val: 276.0336 acc_val: 0.3902 time: 2.4882s\n",
      "Epoch: 0064 loss_train: 27.9699 acc_train: 0.4860 loss_val: 268.7091 acc_val: 0.3902 time: 2.5902s\n",
      "Epoch: 0065 loss_train: 26.3499 acc_train: 0.4825 loss_val: 262.4672 acc_val: 0.3902 time: 2.5282s\n",
      "Epoch: 0066 loss_train: 24.4019 acc_train: 0.4790 loss_val: 256.9590 acc_val: 0.3902 time: 2.5682s\n",
      "Epoch: 0067 loss_train: 22.4121 acc_train: 0.4790 loss_val: 252.1176 acc_val: 0.3902 time: 2.4742s\n",
      "Epoch: 0068 loss_train: 21.3019 acc_train: 0.4773 loss_val: 246.9664 acc_val: 0.4024 time: 2.6882s\n",
      "Epoch: 0069 loss_train: 22.9749 acc_train: 0.4685 loss_val: 238.2958 acc_val: 0.4024 time: 2.6762s\n",
      "Epoch: 0070 loss_train: 22.9790 acc_train: 0.4703 loss_val: 226.3125 acc_val: 0.4024 time: 2.6312s\n",
      "Epoch: 0071 loss_train: 20.2366 acc_train: 0.4773 loss_val: 212.5213 acc_val: 0.4024 time: 2.5422s\n",
      "Epoch: 0072 loss_train: 18.7168 acc_train: 0.4895 loss_val: 200.2924 acc_val: 0.4146 time: 2.4952s\n",
      "Epoch: 0073 loss_train: 19.1970 acc_train: 0.4965 loss_val: 190.5224 acc_val: 0.4268 time: 2.5762s\n",
      "Epoch: 0074 loss_train: 19.4604 acc_train: 0.4983 loss_val: 182.9146 acc_val: 0.4268 time: 2.5472s\n",
      "Epoch: 0075 loss_train: 18.8405 acc_train: 0.4965 loss_val: 177.4156 acc_val: 0.4268 time: 2.5672s\n",
      "Epoch: 0076 loss_train: 17.5041 acc_train: 0.4913 loss_val: 172.8107 acc_val: 0.4390 time: 2.6602s\n",
      "Epoch: 0077 loss_train: 16.5673 acc_train: 0.4895 loss_val: 168.9700 acc_val: 0.4634 time: 3.0012s\n",
      "Epoch: 0078 loss_train: 17.2672 acc_train: 0.4825 loss_val: 162.6697 acc_val: 0.4634 time: 2.7042s\n",
      "Epoch: 0079 loss_train: 17.2534 acc_train: 0.4843 loss_val: 153.9823 acc_val: 0.4634 time: 2.4672s\n",
      "Epoch: 0080 loss_train: 15.7221 acc_train: 0.4843 loss_val: 144.5878 acc_val: 0.4634 time: 2.4562s\n",
      "Epoch: 0081 loss_train: 15.8332 acc_train: 0.4895 loss_val: 136.7805 acc_val: 0.4634 time: 2.5122s\n",
      "Epoch: 0082 loss_train: 16.3108 acc_train: 0.4913 loss_val: 131.5675 acc_val: 0.4512 time: 2.6292s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0083 loss_train: 15.9488 acc_train: 0.4930 loss_val: 128.7782 acc_val: 0.4512 time: 2.5082s\n",
      "Epoch: 0084 loss_train: 14.9682 acc_train: 0.4930 loss_val: 127.0585 acc_val: 0.4634 time: 2.5282s\n",
      "Epoch: 0085 loss_train: 14.7474 acc_train: 0.4878 loss_val: 123.0071 acc_val: 0.4756 time: 2.6712s\n",
      "Epoch: 0086 loss_train: 14.8296 acc_train: 0.4948 loss_val: 116.5164 acc_val: 0.4634 time: 2.8732s\n",
      "Epoch: 0087 loss_train: 14.0276 acc_train: 0.4913 loss_val: 110.1985 acc_val: 0.4756 time: 2.4412s\n",
      "Epoch: 0088 loss_train: 14.1522 acc_train: 0.4948 loss_val: 105.3570 acc_val: 0.4756 time: 2.4112s\n",
      "Epoch: 0089 loss_train: 14.1652 acc_train: 0.4930 loss_val: 101.8155 acc_val: 0.4756 time: 2.4252s\n",
      "Epoch: 0090 loss_train: 13.8681 acc_train: 0.4948 loss_val: 99.4776 acc_val: 0.4634 time: 2.5022s\n",
      "Epoch: 0091 loss_train: 13.3455 acc_train: 0.4895 loss_val: 97.8988 acc_val: 0.4634 time: 2.5052s\n",
      "Epoch: 0092 loss_train: 12.9885 acc_train: 0.4913 loss_val: 95.9035 acc_val: 0.4756 time: 2.6542s\n",
      "Epoch: 0093 loss_train: 13.1429 acc_train: 0.4965 loss_val: 90.8828 acc_val: 0.4512 time: 2.7982s\n",
      "Epoch: 0094 loss_train: 12.5505 acc_train: 0.5035 loss_val: 85.7718 acc_val: 0.4634 time: 2.5872s\n",
      "Epoch: 0095 loss_train: 12.5575 acc_train: 0.5035 loss_val: 81.6538 acc_val: 0.4756 time: 2.5352s\n",
      "Epoch: 0096 loss_train: 12.5969 acc_train: 0.5105 loss_val: 78.7457 acc_val: 0.4878 time: 2.4642s\n",
      "Epoch: 0097 loss_train: 12.3366 acc_train: 0.5140 loss_val: 76.6266 acc_val: 0.4878 time: 2.4442s\n",
      "Epoch: 0098 loss_train: 12.0109 acc_train: 0.5140 loss_val: 75.2310 acc_val: 0.4878 time: 2.4252s\n",
      "Epoch: 0099 loss_train: 11.6987 acc_train: 0.5105 loss_val: 73.1875 acc_val: 0.4878 time: 2.7282s\n",
      "Epoch: 0100 loss_train: 11.6363 acc_train: 0.5157 loss_val: 71.0537 acc_val: 0.5122 time: 2.8042s\n",
      "Epoch: 0101 loss_train: 11.4880 acc_train: 0.5122 loss_val: 69.1504 acc_val: 0.5244 time: 2.6832s\n",
      "Epoch: 0102 loss_train: 11.2487 acc_train: 0.5122 loss_val: 66.8584 acc_val: 0.5244 time: 2.5692s\n",
      "Epoch: 0103 loss_train: 11.0193 acc_train: 0.5192 loss_val: 64.9467 acc_val: 0.5000 time: 2.6642s\n",
      "Epoch: 0104 loss_train: 10.8980 acc_train: 0.5210 loss_val: 63.4933 acc_val: 0.5122 time: 2.6392s\n",
      "Epoch: 0105 loss_train: 10.7000 acc_train: 0.5262 loss_val: 62.4130 acc_val: 0.5122 time: 2.5472s\n",
      "Epoch: 0106 loss_train: 10.4709 acc_train: 0.5280 loss_val: 61.0895 acc_val: 0.5122 time: 2.6482s\n",
      "Epoch: 0107 loss_train: 10.3081 acc_train: 0.5332 loss_val: 59.5936 acc_val: 0.5366 time: 2.9582s\n",
      "Epoch: 0108 loss_train: 10.1714 acc_train: 0.5315 loss_val: 58.5369 acc_val: 0.5366 time: 2.7542s\n",
      "Epoch: 0109 loss_train: 10.0096 acc_train: 0.5315 loss_val: 57.8506 acc_val: 0.5366 time: 2.5122s\n",
      "Epoch: 0110 loss_train: 9.8301 acc_train: 0.5315 loss_val: 56.9681 acc_val: 0.5244 time: 2.4872s\n",
      "Epoch: 0111 loss_train: 9.6798 acc_train: 0.5402 loss_val: 55.9098 acc_val: 0.5122 time: 2.9222s\n",
      "Epoch: 0112 loss_train: 9.6090 acc_train: 0.5437 loss_val: 55.2629 acc_val: 0.5122 time: 2.8502s\n",
      "Epoch: 0113 loss_train: 9.4813 acc_train: 0.5455 loss_val: 54.9426 acc_val: 0.5244 time: 2.8672s\n",
      "Epoch: 0114 loss_train: 9.2847 acc_train: 0.5437 loss_val: 54.7310 acc_val: 0.5366 time: 3.0532s\n",
      "Epoch: 0115 loss_train: 9.2847 acc_train: 0.5455 loss_val: 53.9880 acc_val: 0.5366 time: 2.8102s\n",
      "Epoch: 0116 loss_train: 9.2553 acc_train: 0.5437 loss_val: 52.1539 acc_val: 0.5244 time: 2.6452s\n",
      "Epoch: 0117 loss_train: 9.1278 acc_train: 0.5455 loss_val: 49.9691 acc_val: 0.5244 time: 2.5442s\n",
      "Epoch: 0118 loss_train: 8.8785 acc_train: 0.5507 loss_val: 48.2670 acc_val: 0.5122 time: 2.5952s\n",
      "Epoch: 0119 loss_train: 8.7564 acc_train: 0.5490 loss_val: 47.0507 acc_val: 0.5122 time: 2.5312s\n",
      "Epoch: 0120 loss_train: 8.6859 acc_train: 0.5507 loss_val: 45.6333 acc_val: 0.5122 time: 2.5252s\n",
      "Epoch: 0121 loss_train: 8.6016 acc_train: 0.5542 loss_val: 43.9869 acc_val: 0.5122 time: 2.6052s\n",
      "Epoch: 0122 loss_train: 8.4236 acc_train: 0.5559 loss_val: 43.2286 acc_val: 0.5244 time: 2.8262s\n",
      "Epoch: 0123 loss_train: 8.5512 acc_train: 0.5542 loss_val: 43.6079 acc_val: 0.5244 time: 2.7272s\n",
      "Epoch: 0124 loss_train: 8.4280 acc_train: 0.5577 loss_val: 44.8356 acc_val: 0.5122 time: 2.6982s\n",
      "Epoch: 0125 loss_train: 8.1304 acc_train: 0.5524 loss_val: 45.0555 acc_val: 0.5000 time: 2.6212s\n",
      "Epoch: 0126 loss_train: 8.0889 acc_train: 0.5542 loss_val: 44.2864 acc_val: 0.5122 time: 2.4762s\n",
      "Epoch: 0127 loss_train: 7.9560 acc_train: 0.5559 loss_val: 43.0794 acc_val: 0.5122 time: 2.5282s\n",
      "Epoch: 0128 loss_train: 7.9439 acc_train: 0.5577 loss_val: 42.8429 acc_val: 0.5122 time: 2.7702s\n",
      "Epoch: 0129 loss_train: 7.7774 acc_train: 0.5559 loss_val: 42.1177 acc_val: 0.5122 time: 2.6882s\n",
      "Epoch: 0130 loss_train: 7.7412 acc_train: 0.5577 loss_val: 40.5671 acc_val: 0.5122 time: 2.5952s\n",
      "Epoch: 0131 loss_train: 7.6233 acc_train: 0.5577 loss_val: 38.8567 acc_val: 0.5122 time: 2.6032s\n",
      "Epoch: 0132 loss_train: 7.6029 acc_train: 0.5647 loss_val: 38.3650 acc_val: 0.5122 time: 2.6242s\n",
      "Epoch: 0133 loss_train: 7.4730 acc_train: 0.5629 loss_val: 37.4874 acc_val: 0.5000 time: 2.5742s\n",
      "Epoch: 0134 loss_train: 7.5065 acc_train: 0.5664 loss_val: 33.9217 acc_val: 0.5122 time: 2.4492s\n",
      "Epoch: 0135 loss_train: 7.4340 acc_train: 0.5629 loss_val: 31.7824 acc_val: 0.5122 time: 2.5742s\n",
      "Epoch: 0136 loss_train: 7.4059 acc_train: 0.5682 loss_val: 30.9317 acc_val: 0.5000 time: 2.6602s\n",
      "Epoch: 0137 loss_train: 7.1618 acc_train: 0.5682 loss_val: 31.0485 acc_val: 0.5000 time: 2.9022s\n",
      "Epoch: 0138 loss_train: 7.7238 acc_train: 0.5629 loss_val: 26.5426 acc_val: 0.5000 time: 2.6042s\n",
      "Epoch: 0139 loss_train: 7.0718 acc_train: 0.5682 loss_val: 23.3310 acc_val: 0.5000 time: 2.5312s\n",
      "Epoch: 0140 loss_train: 7.1695 acc_train: 0.5734 loss_val: 22.9694 acc_val: 0.5000 time: 2.4362s\n",
      "Epoch: 0141 loss_train: 6.8591 acc_train: 0.5664 loss_val: 22.0985 acc_val: 0.5000 time: 2.5042s\n",
      "Epoch: 0142 loss_train: 7.2752 acc_train: 0.5769 loss_val: 16.7007 acc_val: 0.5122 time: 2.4902s\n",
      "Epoch: 0143 loss_train: 6.9625 acc_train: 0.5734 loss_val: 14.6583 acc_val: 0.5122 time: 2.4422s\n",
      "Epoch: 0144 loss_train: 6.9403 acc_train: 0.5734 loss_val: 15.6490 acc_val: 0.5000 time: 2.5622s\n",
      "Epoch: 0145 loss_train: 6.7159 acc_train: 0.5612 loss_val: 14.1159 acc_val: 0.5000 time: 2.7602s\n",
      "Epoch: 0146 loss_train: 6.5435 acc_train: 0.5699 loss_val: 12.7420 acc_val: 0.5000 time: 2.6352s\n",
      "Epoch: 0147 loss_train: 6.5194 acc_train: 0.5647 loss_val: 12.2114 acc_val: 0.4878 time: 2.5412s\n",
      "Epoch: 0148 loss_train: 6.4395 acc_train: 0.5647 loss_val: 13.2749 acc_val: 0.5122 time: 2.5472s\n",
      "Epoch: 0149 loss_train: 6.7750 acc_train: 0.5629 loss_val: 11.7890 acc_val: 0.5000 time: 2.8212s\n",
      "Epoch: 0150 loss_train: 6.2567 acc_train: 0.5647 loss_val: 11.5139 acc_val: 0.5000 time: 2.8262s\n",
      "Epoch: 0151 loss_train: 6.5663 acc_train: 0.5664 loss_val: 12.1116 acc_val: 0.4878 time: 2.6202s\n",
      "Epoch: 0152 loss_train: 6.0241 acc_train: 0.5682 loss_val: 12.8504 acc_val: 0.4634 time: 2.6982s\n",
      "Epoch: 0153 loss_train: 6.2169 acc_train: 0.5664 loss_val: 12.0179 acc_val: 0.4878 time: 2.8552s\n",
      "Epoch: 0154 loss_train: 6.0539 acc_train: 0.5682 loss_val: 11.2020 acc_val: 0.4756 time: 2.7052s\n",
      "Epoch: 0155 loss_train: 5.9157 acc_train: 0.5699 loss_val: 10.1281 acc_val: 0.4878 time: 2.4652s\n",
      "Epoch: 0156 loss_train: 5.8659 acc_train: 0.5752 loss_val: 11.1994 acc_val: 0.5000 time: 2.4442s\n",
      "Epoch: 0157 loss_train: 5.8123 acc_train: 0.5647 loss_val: 13.2384 acc_val: 0.5000 time: 2.5252s\n",
      "Epoch: 0158 loss_train: 5.7517 acc_train: 0.5647 loss_val: 14.1563 acc_val: 0.5000 time: 2.6062s\n",
      "Epoch: 0159 loss_train: 5.6950 acc_train: 0.5612 loss_val: 14.4904 acc_val: 0.5000 time: 2.3652s\n",
      "Epoch: 0160 loss_train: 5.7181 acc_train: 0.5682 loss_val: 15.9218 acc_val: 0.4878 time: 2.4562s\n",
      "Epoch: 0161 loss_train: 5.6382 acc_train: 0.5699 loss_val: 16.0155 acc_val: 0.4878 time: 2.5312s\n",
      "Epoch: 0162 loss_train: 5.5709 acc_train: 0.5664 loss_val: 14.9595 acc_val: 0.4878 time: 2.5692s\n",
      "Epoch: 0163 loss_train: 5.6141 acc_train: 0.5734 loss_val: 14.9280 acc_val: 0.4756 time: 2.4752s\n",
      "Epoch: 0164 loss_train: 5.4885 acc_train: 0.5699 loss_val: 15.8551 acc_val: 0.4878 time: 2.4182s\n",
      "Epoch: 0165 loss_train: 5.6790 acc_train: 0.5717 loss_val: 12.8488 acc_val: 0.4878 time: 2.4202s\n",
      "Epoch: 0166 loss_train: 5.4950 acc_train: 0.5647 loss_val: 12.9054 acc_val: 0.5000 time: 2.4472s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0167 loss_train: 5.2791 acc_train: 0.5699 loss_val: 11.8499 acc_val: 0.4878 time: 2.4982s\n",
      "Epoch: 0168 loss_train: 5.2151 acc_train: 0.5699 loss_val: 10.3137 acc_val: 0.4756 time: 2.4782s\n",
      "Epoch: 0169 loss_train: 5.2198 acc_train: 0.5647 loss_val: 10.0247 acc_val: 0.4878 time: 2.6082s\n",
      "Epoch: 0170 loss_train: 5.1910 acc_train: 0.5717 loss_val: 8.4425 acc_val: 0.5000 time: 2.5912s\n",
      "Epoch: 0171 loss_train: 5.0988 acc_train: 0.5734 loss_val: 9.1432 acc_val: 0.5122 time: 2.8392s\n",
      "Epoch: 0172 loss_train: 5.2546 acc_train: 0.5717 loss_val: 8.1476 acc_val: 0.5000 time: 2.4242s\n",
      "Epoch: 0173 loss_train: 5.0424 acc_train: 0.5752 loss_val: 8.0426 acc_val: 0.5122 time: 2.4422s\n",
      "Epoch: 0174 loss_train: 5.0285 acc_train: 0.5787 loss_val: 9.4104 acc_val: 0.5122 time: 2.4772s\n",
      "Epoch: 0175 loss_train: 5.3754 acc_train: 0.5699 loss_val: 8.9871 acc_val: 0.5122 time: 2.6412s\n",
      "Epoch: 0176 loss_train: 5.0598 acc_train: 0.5734 loss_val: 7.0843 acc_val: 0.5122 time: 2.7392s\n",
      "Epoch: 0177 loss_train: 5.9642 acc_train: 0.5769 loss_val: 9.0818 acc_val: 0.5122 time: 2.5952s\n",
      "Epoch: 0178 loss_train: 5.0245 acc_train: 0.5787 loss_val: 9.3155 acc_val: 0.5000 time: 2.8232s\n",
      "Epoch: 0179 loss_train: 5.1316 acc_train: 0.5787 loss_val: 7.8767 acc_val: 0.5000 time: 2.8992s\n",
      "Epoch: 0180 loss_train: 4.8543 acc_train: 0.5839 loss_val: 8.4113 acc_val: 0.5122 time: 2.5942s\n",
      "Epoch: 0181 loss_train: 4.5485 acc_train: 0.5892 loss_val: 8.7148 acc_val: 0.5122 time: 2.5012s\n",
      "Epoch: 0182 loss_train: 4.6366 acc_train: 0.5857 loss_val: 7.8469 acc_val: 0.4878 time: 2.3992s\n",
      "Epoch: 0183 loss_train: 4.8153 acc_train: 0.5787 loss_val: 9.7331 acc_val: 0.5122 time: 2.6632s\n",
      "Epoch: 0184 loss_train: 5.1733 acc_train: 0.5734 loss_val: 9.6876 acc_val: 0.5000 time: 2.7922s\n",
      "Epoch: 0185 loss_train: 5.1141 acc_train: 0.5787 loss_val: 8.0059 acc_val: 0.5000 time: 2.6072s\n",
      "Epoch: 0186 loss_train: 4.5431 acc_train: 0.5822 loss_val: 8.2566 acc_val: 0.5000 time: 2.5962s\n",
      "Epoch: 0187 loss_train: 4.3660 acc_train: 0.5839 loss_val: 9.2306 acc_val: 0.4878 time: 2.6552s\n",
      "Epoch: 0188 loss_train: 4.6703 acc_train: 0.5822 loss_val: 8.4392 acc_val: 0.4878 time: 2.5882s\n",
      "Epoch: 0189 loss_train: 4.3006 acc_train: 0.5909 loss_val: 7.8918 acc_val: 0.5000 time: 2.4642s\n",
      "Epoch: 0190 loss_train: 4.5484 acc_train: 0.5857 loss_val: 10.1871 acc_val: 0.4878 time: 2.4792s\n",
      "Epoch: 0191 loss_train: 5.2400 acc_train: 0.5857 loss_val: 10.4781 acc_val: 0.4878 time: 2.6923s\n",
      "Epoch: 0192 loss_train: 5.4274 acc_train: 0.5874 loss_val: 8.9390 acc_val: 0.5000 time: 2.5302s\n",
      "Epoch: 0193 loss_train: 4.3527 acc_train: 0.5927 loss_val: 6.4440 acc_val: 0.5000 time: 2.8192s\n",
      "Epoch: 0194 loss_train: 6.7629 acc_train: 0.5927 loss_val: 9.3596 acc_val: 0.4878 time: 2.4552s\n",
      "Epoch: 0195 loss_train: 4.4725 acc_train: 0.5857 loss_val: 10.3574 acc_val: 0.4878 time: 2.4332s\n",
      "Epoch: 0196 loss_train: 5.1165 acc_train: 0.5769 loss_val: 9.3345 acc_val: 0.4878 time: 2.4292s\n",
      "Epoch: 0197 loss_train: 4.3797 acc_train: 0.5909 loss_val: 6.9261 acc_val: 0.5000 time: 2.4822s\n",
      "Epoch: 0198 loss_train: 5.7845 acc_train: 0.5979 loss_val: 9.3955 acc_val: 0.5000 time: 2.4472s\n",
      "Epoch: 0199 loss_train: 4.4194 acc_train: 0.5892 loss_val: 10.0433 acc_val: 0.5000 time: 2.7522s\n",
      "Epoch: 0200 loss_train: 4.8601 acc_train: 0.5839 loss_val: 8.8239 acc_val: 0.4878 time: 3.0042s\n",
      "Epoch: 0201 loss_train: 4.1117 acc_train: 0.6031 loss_val: 6.8880 acc_val: 0.4878 time: 2.6482s\n",
      "Epoch: 0202 loss_train: 5.6410 acc_train: 0.5944 loss_val: 9.9207 acc_val: 0.4756 time: 2.5032s\n",
      "Epoch: 0203 loss_train: 4.6640 acc_train: 0.5857 loss_val: 10.8050 acc_val: 0.4756 time: 2.4542s\n",
      "Epoch: 0204 loss_train: 5.2877 acc_train: 0.5874 loss_val: 9.6771 acc_val: 0.4878 time: 2.5582s\n",
      "Epoch: 0205 loss_train: 4.4968 acc_train: 0.5927 loss_val: 6.9848 acc_val: 0.4756 time: 2.5912s\n",
      "Epoch: 0206 loss_train: 5.3204 acc_train: 0.5892 loss_val: 8.2351 acc_val: 0.4878 time: 2.5432s\n",
      "Epoch: 0207 loss_train: 3.9105 acc_train: 0.5944 loss_val: 9.7942 acc_val: 0.4878 time: 2.4052s\n",
      "Epoch: 0208 loss_train: 4.5106 acc_train: 0.5839 loss_val: 9.3073 acc_val: 0.4878 time: 2.5152s\n",
      "Epoch: 0209 loss_train: 4.2021 acc_train: 0.5874 loss_val: 7.4244 acc_val: 0.4878 time: 2.6082s\n",
      "Epoch: 0210 loss_train: 4.4173 acc_train: 0.5997 loss_val: 8.8674 acc_val: 0.5000 time: 2.5442s\n",
      "Epoch: 0211 loss_train: 3.9762 acc_train: 0.6014 loss_val: 9.2789 acc_val: 0.5000 time: 2.4732s\n",
      "Epoch: 0212 loss_train: 4.1578 acc_train: 0.5874 loss_val: 8.0611 acc_val: 0.5000 time: 2.5812s\n",
      "Epoch: 0213 loss_train: 3.7880 acc_train: 0.5997 loss_val: 7.5593 acc_val: 0.4878 time: 2.5822s\n",
      "Epoch: 0214 loss_train: 4.0421 acc_train: 0.5979 loss_val: 9.8454 acc_val: 0.4878 time: 2.6132s\n",
      "Epoch: 0215 loss_train: 4.5768 acc_train: 0.5892 loss_val: 10.0581 acc_val: 0.5000 time: 2.6032s\n",
      "Epoch: 0216 loss_train: 4.7540 acc_train: 0.5857 loss_val: 8.3946 acc_val: 0.5000 time: 2.6162s\n",
      "Epoch: 0217 loss_train: 3.7877 acc_train: 0.5944 loss_val: 6.2685 acc_val: 0.4878 time: 2.4742s\n",
      "Epoch: 0218 loss_train: 5.9242 acc_train: 0.5979 loss_val: 9.7072 acc_val: 0.4878 time: 2.4122s\n",
      "Epoch: 0219 loss_train: 4.3695 acc_train: 0.5804 loss_val: 11.0222 acc_val: 0.4878 time: 2.4922s\n",
      "Epoch: 0220 loss_train: 5.2535 acc_train: 0.5787 loss_val: 10.1402 acc_val: 0.4878 time: 2.4002s\n",
      "Epoch: 0221 loss_train: 4.5563 acc_train: 0.5857 loss_val: 7.5117 acc_val: 0.4756 time: 2.4142s\n",
      "Epoch: 0222 loss_train: 4.0903 acc_train: 0.5997 loss_val: 7.9790 acc_val: 0.4756 time: 2.5052s\n",
      "Epoch: 0223 loss_train: 3.6538 acc_train: 0.6014 loss_val: 9.1455 acc_val: 0.4878 time: 2.5712s\n",
      "Epoch: 0224 loss_train: 3.9392 acc_train: 0.5839 loss_val: 8.8148 acc_val: 0.5000 time: 2.5682s\n",
      "Epoch: 0225 loss_train: 3.7926 acc_train: 0.5962 loss_val: 7.5358 acc_val: 0.4878 time: 2.5292s\n",
      "Epoch: 0226 loss_train: 3.7143 acc_train: 0.5944 loss_val: 8.6913 acc_val: 0.4878 time: 2.5542s\n",
      "Epoch: 0227 loss_train: 3.7104 acc_train: 0.5874 loss_val: 9.0062 acc_val: 0.4878 time: 2.5742s\n",
      "Epoch: 0228 loss_train: 3.7812 acc_train: 0.5857 loss_val: 8.1115 acc_val: 0.4634 time: 2.4722s\n",
      "Epoch: 0229 loss_train: 3.5095 acc_train: 0.5962 loss_val: 7.3986 acc_val: 0.4634 time: 2.4122s\n",
      "Epoch: 0230 loss_train: 3.8473 acc_train: 0.5944 loss_val: 9.5998 acc_val: 0.4878 time: 2.7402s\n",
      "Epoch: 0231 loss_train: 4.1846 acc_train: 0.5804 loss_val: 9.6640 acc_val: 0.5000 time: 2.8012s\n",
      "Epoch: 0232 loss_train: 4.2664 acc_train: 0.5822 loss_val: 7.8059 acc_val: 0.5000 time: 2.8822s\n",
      "Epoch: 0233 loss_train: 3.4542 acc_train: 0.6014 loss_val: 6.6794 acc_val: 0.4634 time: 2.4502s\n",
      "Epoch: 0234 loss_train: 4.5007 acc_train: 0.5892 loss_val: 9.7020 acc_val: 0.4878 time: 2.5052s\n",
      "Epoch: 0235 loss_train: 4.2679 acc_train: 0.5717 loss_val: 10.6078 acc_val: 0.4878 time: 2.5862s\n",
      "Epoch: 0236 loss_train: 4.8936 acc_train: 0.5752 loss_val: 9.5625 acc_val: 0.5000 time: 2.5142s\n",
      "Epoch: 0237 loss_train: 4.1285 acc_train: 0.5699 loss_val: 6.9343 acc_val: 0.4756 time: 2.4502s\n",
      "Epoch: 0238 loss_train: 4.2088 acc_train: 0.5892 loss_val: 8.0990 acc_val: 0.4878 time: 2.7962s\n",
      "Epoch: 0239 loss_train: 3.4084 acc_train: 0.6031 loss_val: 8.6915 acc_val: 0.4878 time: 2.8152s\n",
      "Epoch: 0240 loss_train: 3.5623 acc_train: 0.5979 loss_val: 8.3477 acc_val: 0.4878 time: 2.5482s\n",
      "Epoch: 0241 loss_train: 3.4235 acc_train: 0.6101 loss_val: 7.5190 acc_val: 0.4878 time: 2.4732s\n",
      "Epoch: 0242 loss_train: 3.5132 acc_train: 0.6066 loss_val: 9.6519 acc_val: 0.4878 time: 2.4532s\n",
      "Epoch: 0243 loss_train: 3.9062 acc_train: 0.5874 loss_val: 9.9113 acc_val: 0.4634 time: 2.4412s\n",
      "Epoch: 0244 loss_train: 4.0257 acc_train: 0.5822 loss_val: 8.2155 acc_val: 0.4634 time: 2.4342s\n",
      "Epoch: 0245 loss_train: 3.2992 acc_train: 0.6049 loss_val: 6.0781 acc_val: 0.4634 time: 2.5802s\n",
      "Epoch: 0246 loss_train: 6.2360 acc_train: 0.5909 loss_val: 10.2158 acc_val: 0.5000 time: 2.7002s\n",
      "Epoch: 0247 loss_train: 4.2944 acc_train: 0.5857 loss_val: 12.0956 acc_val: 0.5122 time: 2.5682s\n",
      "Epoch: 0248 loss_train: 5.9956 acc_train: 0.5787 loss_val: 10.4614 acc_val: 0.5122 time: 2.4912s\n",
      "Epoch: 0249 loss_train: 4.5759 acc_train: 0.5892 loss_val: 7.2650 acc_val: 0.5122 time: 2.4942s\n",
      "Epoch: 0250 loss_train: 3.9323 acc_train: 0.5979 loss_val: 7.5231 acc_val: 0.4878 time: 2.5422s\n",
      "Epoch: 0251 loss_train: 3.8099 acc_train: 0.5979 loss_val: 10.4438 acc_val: 0.4878 time: 2.5192s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0252 loss_train: 4.5341 acc_train: 0.5787 loss_val: 15.9744 acc_val: 0.4878 time: 2.5452s\n",
      "Epoch: 0253 loss_train: 5.0489 acc_train: 0.5857 loss_val: 25.6492 acc_val: 0.5000 time: 2.5742s\n",
      "Epoch: 0254 loss_train: 4.1314 acc_train: 0.6014 loss_val: 36.7653 acc_val: 0.4390 time: 2.7162s\n",
      "Epoch: 0255 loss_train: 5.2435 acc_train: 0.5857 loss_val: 38.0584 acc_val: 0.4634 time: 2.7262s\n",
      "Epoch: 0256 loss_train: 3.7735 acc_train: 0.5892 loss_val: 41.4134 acc_val: 0.4512 time: 2.6802s\n",
      "Epoch: 0257 loss_train: 4.1466 acc_train: 0.5822 loss_val: 46.9496 acc_val: 0.4756 time: 2.6172s\n",
      "Epoch: 0258 loss_train: 3.7233 acc_train: 0.5997 loss_val: 51.3347 acc_val: 0.5000 time: 2.5182s\n",
      "Epoch: 0259 loss_train: 4.0041 acc_train: 0.5944 loss_val: 50.4716 acc_val: 0.5122 time: 2.4092s\n",
      "Epoch: 0260 loss_train: 4.0500 acc_train: 0.5909 loss_val: 52.5312 acc_val: 0.5000 time: 2.4972s\n",
      "Epoch: 0261 loss_train: 3.9601 acc_train: 0.5927 loss_val: 57.2779 acc_val: 0.4878 time: 2.7012s\n",
      "Epoch: 0262 loss_train: 4.0300 acc_train: 0.5979 loss_val: 56.7709 acc_val: 0.5000 time: 2.7692s\n",
      "Epoch: 0263 loss_train: 3.6827 acc_train: 0.5997 loss_val: 55.7173 acc_val: 0.5000 time: 2.8232s\n",
      "Epoch: 0264 loss_train: 3.9365 acc_train: 0.5909 loss_val: 57.6152 acc_val: 0.5122 time: 2.6362s\n",
      "Epoch: 0265 loss_train: 3.6415 acc_train: 0.6014 loss_val: 58.8722 acc_val: 0.5000 time: 2.5782s\n",
      "Epoch: 0266 loss_train: 3.8125 acc_train: 0.5979 loss_val: 56.9188 acc_val: 0.5000 time: 2.4382s\n",
      "Epoch: 0267 loss_train: 3.5674 acc_train: 0.5979 loss_val: 54.5452 acc_val: 0.4878 time: 2.4282s\n",
      "Epoch: 0268 loss_train: 3.8627 acc_train: 0.5839 loss_val: 54.9173 acc_val: 0.5000 time: 2.4522s\n",
      "Epoch: 0269 loss_train: 3.4556 acc_train: 0.5997 loss_val: 54.6725 acc_val: 0.4878 time: 2.5992s\n",
      "Epoch: 0270 loss_train: 6.0915 acc_train: 0.6031 loss_val: 56.9535 acc_val: 0.5122 time: 2.6222s\n",
      "Epoch: 0271 loss_train: 3.5573 acc_train: 0.6014 loss_val: 58.0554 acc_val: 0.5000 time: 2.7582s\n",
      "Epoch: 0272 loss_train: 3.5658 acc_train: 0.5892 loss_val: 58.9266 acc_val: 0.5000 time: 2.4452s\n",
      "Epoch: 0273 loss_train: 3.8121 acc_train: 0.5787 loss_val: 62.6892 acc_val: 0.4756 time: 2.4212s\n",
      "Epoch: 0274 loss_train: 3.7478 acc_train: 0.5962 loss_val: 63.2943 acc_val: 0.4878 time: 2.4532s\n",
      "Epoch: 0275 loss_train: 3.7042 acc_train: 0.5997 loss_val: 61.3572 acc_val: 0.4878 time: 2.5952s\n",
      "Epoch: 0276 loss_train: 3.9549 acc_train: 0.5822 loss_val: 62.6444 acc_val: 0.4878 time: 2.5452s\n",
      "Epoch: 0277 loss_train: 3.6705 acc_train: 0.5752 loss_val: 64.5029 acc_val: 0.4878 time: 2.7612s\n",
      "Epoch: 0278 loss_train: 4.0066 acc_train: 0.5857 loss_val: 61.2797 acc_val: 0.5122 time: 2.7672s\n",
      "Epoch: 0279 loss_train: 3.7167 acc_train: 0.5857 loss_val: 59.2516 acc_val: 0.5122 time: 2.7452s\n",
      "Epoch: 0280 loss_train: 4.0456 acc_train: 0.5857 loss_val: 60.3848 acc_val: 0.5122 time: 2.4402s\n",
      "Epoch: 0281 loss_train: 3.5515 acc_train: 0.5857 loss_val: 61.0651 acc_val: 0.4756 time: 2.5582s\n",
      "Epoch: 0282 loss_train: 4.1778 acc_train: 0.5804 loss_val: 54.8515 acc_val: 0.4878 time: 2.5982s\n",
      "Epoch: 0283 loss_train: 4.3816 acc_train: 0.5822 loss_val: 52.6795 acc_val: 0.5000 time: 2.5182s\n",
      "Epoch: 0284 loss_train: 4.6741 acc_train: 0.5804 loss_val: 54.2699 acc_val: 0.5122 time: 2.5422s\n",
      "Epoch: 0285 loss_train: 3.5283 acc_train: 0.5909 loss_val: 56.3311 acc_val: 0.5000 time: 2.6412s\n",
      "Epoch: 0286 loss_train: 5.0989 acc_train: 0.5892 loss_val: 50.2767 acc_val: 0.5000 time: 2.7382s\n",
      "Epoch: 0287 loss_train: 3.5310 acc_train: 0.5752 loss_val: 47.5357 acc_val: 0.5000 time: 2.5542s\n",
      "Epoch: 0288 loss_train: 3.7762 acc_train: 0.5769 loss_val: 47.9143 acc_val: 0.4878 time: 2.4752s\n",
      "Epoch: 0289 loss_train: 3.3491 acc_train: 0.5997 loss_val: 45.4723 acc_val: 0.5000 time: 2.5052s\n",
      "Epoch: 0290 loss_train: 3.2943 acc_train: 0.5997 loss_val: 42.6878 acc_val: 0.5000 time: 2.7112s\n",
      "Epoch: 0291 loss_train: 3.2430 acc_train: 0.5892 loss_val: 39.7632 acc_val: 0.5122 time: 2.5122s\n",
      "Epoch: 0292 loss_train: 3.2257 acc_train: 0.5839 loss_val: 37.3094 acc_val: 0.5122 time: 2.7392s\n",
      "Epoch: 0293 loss_train: 3.1868 acc_train: 0.5839 loss_val: 35.2916 acc_val: 0.5122 time: 2.7022s\n",
      "Epoch: 0294 loss_train: 3.1535 acc_train: 0.5962 loss_val: 33.0995 acc_val: 0.5122 time: 2.6692s\n",
      "Epoch: 0295 loss_train: 3.1118 acc_train: 0.5927 loss_val: 30.7323 acc_val: 0.5000 time: 2.5222s\n",
      "Epoch: 0296 loss_train: 3.0670 acc_train: 0.5979 loss_val: 28.3338 acc_val: 0.5000 time: 2.5602s\n",
      "Epoch: 0297 loss_train: 3.0303 acc_train: 0.5962 loss_val: 25.6938 acc_val: 0.5000 time: 2.5372s\n",
      "Epoch: 0298 loss_train: 3.0385 acc_train: 0.5962 loss_val: 24.9936 acc_val: 0.5122 time: 2.5442s\n",
      "Epoch: 0299 loss_train: 3.0436 acc_train: 0.5962 loss_val: 23.2175 acc_val: 0.5000 time: 2.4492s\n",
      "Epoch: 0300 loss_train: 3.1049 acc_train: 0.5979 loss_val: 17.8297 acc_val: 0.5000 time: 2.8852s\n",
      "Epoch: 0301 loss_train: 3.4012 acc_train: 0.5804 loss_val: 16.2766 acc_val: 0.5000 time: 3.0202s\n",
      "Epoch: 0302 loss_train: 3.1844 acc_train: 0.5927 loss_val: 17.5204 acc_val: 0.5000 time: 2.6672s\n",
      "Epoch: 0303 loss_train: 3.4586 acc_train: 0.6031 loss_val: 10.7647 acc_val: 0.5000 time: 2.6352s\n",
      "Epoch: 0304 loss_train: 3.4975 acc_train: 0.5839 loss_val: 10.0087 acc_val: 0.5000 time: 2.6332s\n",
      "Epoch: 0305 loss_train: 3.6909 acc_train: 0.5822 loss_val: 9.0809 acc_val: 0.4878 time: 2.5322s\n",
      "Epoch: 0306 loss_train: 2.7897 acc_train: 0.6014 loss_val: 8.7602 acc_val: 0.4878 time: 2.5472s\n",
      "Epoch: 0307 loss_train: 3.5631 acc_train: 0.5927 loss_val: 11.2671 acc_val: 0.5000 time: 2.5992s\n",
      "Epoch: 0308 loss_train: 4.5270 acc_train: 0.5857 loss_val: 12.8387 acc_val: 0.5122 time: 2.6092s\n",
      "Epoch: 0309 loss_train: 5.7052 acc_train: 0.5787 loss_val: 11.7619 acc_val: 0.5244 time: 2.4592s\n",
      "Epoch: 0310 loss_train: 4.9017 acc_train: 0.5822 loss_val: 8.6281 acc_val: 0.5122 time: 2.4462s\n",
      "Epoch: 0311 loss_train: 2.8852 acc_train: 0.5979 loss_val: 5.2221 acc_val: 0.4878 time: 2.5332s\n",
      "Epoch: 0312 loss_train: 8.6479 acc_train: 0.5839 loss_val: 9.4899 acc_val: 0.5122 time: 2.4992s\n",
      "Epoch: 0313 loss_train: 3.3252 acc_train: 0.5874 loss_val: 11.9212 acc_val: 0.5366 time: 2.4812s\n",
      "Epoch: 0314 loss_train: 5.0055 acc_train: 0.5734 loss_val: 11.9103 acc_val: 0.5366 time: 2.5322s\n",
      "Epoch: 0315 loss_train: 4.9537 acc_train: 0.5734 loss_val: 9.6597 acc_val: 0.5122 time: 2.7072s\n",
      "Epoch: 0316 loss_train: 3.2735 acc_train: 0.5927 loss_val: 6.0260 acc_val: 0.4878 time: 2.7502s\n",
      "Epoch: 0317 loss_train: 6.0012 acc_train: 0.5962 loss_val: 9.9103 acc_val: 0.4878 time: 2.6362s\n",
      "Epoch: 0318 loss_train: 3.2985 acc_train: 0.5857 loss_val: 11.5670 acc_val: 0.5122 time: 2.5401s\n",
      "Epoch: 0319 loss_train: 4.5575 acc_train: 0.5664 loss_val: 10.6358 acc_val: 0.5244 time: 2.4182s\n",
      "Epoch: 0320 loss_train: 3.7834 acc_train: 0.5874 loss_val: 7.6375 acc_val: 0.5122 time: 2.4462s\n",
      "Epoch: 0321 loss_train: 2.7869 acc_train: 0.6224 loss_val: 7.1066 acc_val: 0.4878 time: 2.3912s\n",
      "Epoch: 0322 loss_train: 3.4336 acc_train: 0.6084 loss_val: 11.0259 acc_val: 0.5000 time: 2.4592s\n",
      "Epoch: 0323 loss_train: 4.0605 acc_train: 0.5787 loss_val: 12.4762 acc_val: 0.5122 time: 2.5782s\n",
      "Epoch: 0324 loss_train: 5.1048 acc_train: 0.5822 loss_val: 11.5622 acc_val: 0.5000 time: 2.5902s\n",
      "Epoch: 0325 loss_train: 4.4173 acc_train: 0.5892 loss_val: 8.6863 acc_val: 0.5000 time: 2.7332s\n",
      "Epoch: 0326 loss_train: 2.7434 acc_train: 0.6136 loss_val: 5.6871 acc_val: 0.4878 time: 2.5012s\n",
      "Epoch: 0327 loss_train: 6.8620 acc_train: 0.5892 loss_val: 10.3973 acc_val: 0.5122 time: 2.4872s\n",
      "Epoch: 0328 loss_train: 3.4587 acc_train: 0.5892 loss_val: 12.7357 acc_val: 0.5000 time: 2.4122s\n",
      "Epoch: 0329 loss_train: 5.3169 acc_train: 0.5647 loss_val: 12.3634 acc_val: 0.5122 time: 2.5542s\n",
      "Epoch: 0330 loss_train: 4.9339 acc_train: 0.5927 loss_val: 9.7197 acc_val: 0.4878 time: 2.5382s\n",
      "Epoch: 0331 loss_train: 3.0601 acc_train: 0.6049 loss_val: 5.9600 acc_val: 0.4756 time: 2.6772s\n",
      "Epoch: 0332 loss_train: 6.2707 acc_train: 0.5839 loss_val: 9.8474 acc_val: 0.5000 time: 2.8562s\n",
      "Epoch: 0333 loss_train: 3.1348 acc_train: 0.5962 loss_val: 11.8676 acc_val: 0.5000 time: 2.7812s\n",
      "Epoch: 0334 loss_train: 4.4975 acc_train: 0.5734 loss_val: 11.5846 acc_val: 0.5000 time: 2.5652s\n",
      "Epoch: 0335 loss_train: 4.2561 acc_train: 0.5927 loss_val: 9.2577 acc_val: 0.5000 time: 2.5142s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0336 loss_train: 2.8389 acc_train: 0.6119 loss_val: 6.5793 acc_val: 0.4756 time: 2.4602s\n",
      "Epoch: 0337 loss_train: 4.5660 acc_train: 0.5962 loss_val: 10.8023 acc_val: 0.4878 time: 2.5432s\n",
      "Epoch: 0338 loss_train: 3.5391 acc_train: 0.5647 loss_val: 12.4403 acc_val: 0.4878 time: 2.5832s\n",
      "Epoch: 0339 loss_train: 4.8560 acc_train: 0.5664 loss_val: 11.3912 acc_val: 0.5122 time: 2.6352s\n",
      "Epoch: 0340 loss_train: 3.9615 acc_train: 0.5979 loss_val: 8.2792 acc_val: 0.4878 time: 2.7062s\n",
      "Epoch: 0341 loss_train: 2.6501 acc_train: 0.6084 loss_val: 5.2201 acc_val: 0.5000 time: 2.7832s\n",
      "Epoch: 0342 loss_train: 7.9220 acc_train: 0.5839 loss_val: 10.1634 acc_val: 0.5122 time: 2.5042s\n",
      "Epoch: 0343 loss_train: 3.2451 acc_train: 0.5857 loss_val: 13.1245 acc_val: 0.4756 time: 2.4302s\n",
      "Epoch: 0344 loss_train: 5.3592 acc_train: 0.5682 loss_val: 13.5191 acc_val: 0.4634 time: 2.5122s\n",
      "Epoch: 0345 loss_train: 5.5659 acc_train: 0.5752 loss_val: 11.5777 acc_val: 0.5122 time: 2.3972s\n",
      "Epoch: 0346 loss_train: 4.1284 acc_train: 0.5892 loss_val: 7.9165 acc_val: 0.4756 time: 2.4352s\n",
      "Epoch: 0347 loss_train: 2.6930 acc_train: 0.6049 loss_val: 5.8987 acc_val: 0.4756 time: 2.5722s\n",
      "Epoch: 0348 loss_train: 6.3835 acc_train: 0.5874 loss_val: 11.0201 acc_val: 0.5000 time: 2.6102s\n",
      "Epoch: 0349 loss_train: 3.5465 acc_train: 0.5822 loss_val: 13.6134 acc_val: 0.5000 time: 2.5542s\n",
      "Epoch: 0350 loss_train: 5.6686 acc_train: 0.5769 loss_val: 13.4488 acc_val: 0.5000 time: 2.5002s\n",
      "Epoch: 0351 loss_train: 5.5075 acc_train: 0.5857 loss_val: 10.8886 acc_val: 0.5122 time: 2.5672s\n",
      "Epoch: 0352 loss_train: 3.5810 acc_train: 0.5892 loss_val: 6.7227 acc_val: 0.4756 time: 2.4562s\n",
      "Epoch: 0353 loss_train: 3.9894 acc_train: 0.5997 loss_val: 8.0692 acc_val: 0.4878 time: 2.5022s\n",
      "Epoch: 0354 loss_train: 2.6362 acc_train: 0.6031 loss_val: 8.8684 acc_val: 0.5000 time: 2.4442s\n",
      "Epoch: 0355 loss_train: 2.7461 acc_train: 0.5962 loss_val: 9.0621 acc_val: 0.5000 time: 2.5362s\n",
      "Epoch: 0356 loss_train: 2.7779 acc_train: 0.5944 loss_val: 8.7358 acc_val: 0.5000 time: 2.8292s\n",
      "Epoch: 0357 loss_train: 2.7233 acc_train: 0.6031 loss_val: 8.0326 acc_val: 0.5000 time: 2.5842s\n",
      "Epoch: 0358 loss_train: 2.5601 acc_train: 0.6049 loss_val: 7.1085 acc_val: 0.4756 time: 2.4852s\n",
      "Epoch: 0359 loss_train: 3.0456 acc_train: 0.6014 loss_val: 10.0753 acc_val: 0.4878 time: 2.7012s\n",
      "Epoch: 0360 loss_train: 3.0589 acc_train: 0.5892 loss_val: 11.1475 acc_val: 0.5000 time: 2.5302s\n",
      "Epoch: 0361 loss_train: 3.7972 acc_train: 0.5857 loss_val: 9.8897 acc_val: 0.4878 time: 2.4752s\n",
      "Epoch: 0362 loss_train: 2.9576 acc_train: 0.5927 loss_val: 7.0782 acc_val: 0.4756 time: 2.4832s\n",
      "Epoch: 0363 loss_train: 3.0147 acc_train: 0.6066 loss_val: 8.7982 acc_val: 0.4756 time: 2.5182s\n",
      "Epoch: 0364 loss_train: 2.5214 acc_train: 0.6031 loss_val: 9.7747 acc_val: 0.5000 time: 2.7002s\n",
      "Epoch: 0365 loss_train: 2.9406 acc_train: 0.5892 loss_val: 8.9615 acc_val: 0.5000 time: 2.4142s\n",
      "Epoch: 0366 loss_train: 2.5884 acc_train: 0.6014 loss_val: 7.4379 acc_val: 0.4878 time: 2.5452s\n",
      "Epoch: 0367 loss_train: 2.4074 acc_train: 0.6154 loss_val: 6.2150 acc_val: 0.4878 time: 2.6062s\n",
      "Epoch: 0368 loss_train: 3.8535 acc_train: 0.6014 loss_val: 10.5050 acc_val: 0.5000 time: 2.5622s\n",
      "Epoch: 0369 loss_train: 3.5803 acc_train: 0.5892 loss_val: 12.4132 acc_val: 0.5000 time: 2.4272s\n",
      "Epoch: 0370 loss_train: 5.0264 acc_train: 0.5752 loss_val: 11.6842 acc_val: 0.5000 time: 2.5202s\n",
      "Epoch: 0371 loss_train: 4.4618 acc_train: 0.5892 loss_val: 8.8974 acc_val: 0.4878 time: 2.4062s\n",
      "Epoch: 0372 loss_train: 2.6732 acc_train: 0.6014 loss_val: 5.3186 acc_val: 0.4756 time: 2.4032s\n",
      "Epoch: 0373 loss_train: 6.2788 acc_train: 0.5857 loss_val: 9.2882 acc_val: 0.5000 time: 2.4732s\n",
      "Epoch: 0374 loss_train: 2.8736 acc_train: 0.5909 loss_val: 11.4827 acc_val: 0.4878 time: 2.4152s\n",
      "Epoch: 0375 loss_train: 4.3878 acc_train: 0.5664 loss_val: 11.2552 acc_val: 0.5122 time: 2.4992s\n",
      "Epoch: 0376 loss_train: 4.1798 acc_train: 0.5804 loss_val: 8.9527 acc_val: 0.5000 time: 2.5192s\n",
      "Epoch: 0377 loss_train: 2.6227 acc_train: 0.6049 loss_val: 5.7924 acc_val: 0.4756 time: 2.5082s\n",
      "Epoch: 0378 loss_train: 4.7729 acc_train: 0.5997 loss_val: 9.6960 acc_val: 0.5000 time: 2.4192s\n",
      "Epoch: 0379 loss_train: 2.9097 acc_train: 0.5944 loss_val: 11.5428 acc_val: 0.5122 time: 2.6852s\n",
      "Epoch: 0380 loss_train: 4.8407 acc_train: 0.5734 loss_val: 9.6035 acc_val: 0.5000 time: 2.6132s\n",
      "Epoch: 0381 loss_train: 2.8797 acc_train: 0.6014 loss_val: 6.3165 acc_val: 0.5000 time: 2.7402s\n",
      "Epoch: 0382 loss_train: 3.5780 acc_train: 0.6014 loss_val: 7.7391 acc_val: 0.5000 time: 2.5362s\n",
      "Epoch: 0383 loss_train: 2.4042 acc_train: 0.6066 loss_val: 9.0850 acc_val: 0.5000 time: 2.4532s\n",
      "Epoch: 0384 loss_train: 2.6359 acc_train: 0.5962 loss_val: 17.9481 acc_val: 0.5000 time: 2.5662s\n",
      "Epoch: 0385 loss_train: 2.6376 acc_train: 0.5892 loss_val: 27.6122 acc_val: 0.5000 time: 2.5562s\n",
      "Epoch: 0386 loss_train: 2.5103 acc_train: 0.5997 loss_val: 34.1852 acc_val: 0.4878 time: 2.7262s\n",
      "Epoch: 0387 loss_train: 2.5693 acc_train: 0.5979 loss_val: 38.1822 acc_val: 0.5000 time: 2.6172s\n",
      "Epoch: 0388 loss_train: 2.6407 acc_train: 0.5892 loss_val: 43.3531 acc_val: 0.5000 time: 2.7162s\n",
      "Epoch: 0389 loss_train: 2.5846 acc_train: 0.5892 loss_val: 49.3057 acc_val: 0.5122 time: 2.5472s\n",
      "Epoch: 0390 loss_train: 2.7249 acc_train: 0.5909 loss_val: 52.7739 acc_val: 0.5122 time: 2.4762s\n",
      "Epoch: 0391 loss_train: 2.6757 acc_train: 0.5962 loss_val: 54.3029 acc_val: 0.5000 time: 2.4242s\n",
      "Epoch: 0392 loss_train: 2.6576 acc_train: 0.5979 loss_val: 57.4724 acc_val: 0.5000 time: 2.5232s\n",
      "Epoch: 0393 loss_train: 2.6048 acc_train: 0.5997 loss_val: 59.3838 acc_val: 0.5122 time: 2.5662s\n",
      "Epoch: 0394 loss_train: 2.5959 acc_train: 0.5997 loss_val: 61.4223 acc_val: 0.5000 time: 2.7902s\n",
      "Epoch: 0395 loss_train: 2.6248 acc_train: 0.5962 loss_val: 62.3067 acc_val: 0.5000 time: 2.6272s\n",
      "Epoch: 0396 loss_train: 2.6169 acc_train: 0.5874 loss_val: 62.2801 acc_val: 0.5000 time: 2.6412s\n",
      "Epoch: 0397 loss_train: 2.7122 acc_train: 0.5769 loss_val: 64.3118 acc_val: 0.5000 time: 2.4402s\n",
      "Epoch: 0398 loss_train: 2.6557 acc_train: 0.5909 loss_val: 65.0464 acc_val: 0.4878 time: 2.3802s\n",
      "Epoch: 0399 loss_train: 2.7502 acc_train: 0.5927 loss_val: 63.6962 acc_val: 0.5000 time: 2.4202s\n",
      "Epoch: 0400 loss_train: 2.5813 acc_train: 0.5892 loss_val: 61.6489 acc_val: 0.5000 time: 2.4132s\n",
      "Epoch: 0401 loss_train: 2.8679 acc_train: 0.5804 loss_val: 62.8482 acc_val: 0.5000 time: 2.5832s\n",
      "Epoch: 0402 loss_train: 2.5575 acc_train: 0.5979 loss_val: 63.0806 acc_val: 0.5000 time: 2.7272s\n",
      "Epoch: 0403 loss_train: 2.6428 acc_train: 0.5944 loss_val: 61.6743 acc_val: 0.5000 time: 2.5922s\n",
      "Epoch: 0404 loss_train: 2.5607 acc_train: 0.5962 loss_val: 59.4380 acc_val: 0.5244 time: 2.5622s\n",
      "Epoch: 0405 loss_train: 2.5701 acc_train: 0.5997 loss_val: 59.2147 acc_val: 0.5122 time: 2.5082s\n",
      "Epoch: 0406 loss_train: 2.5096 acc_train: 0.6014 loss_val: 58.4650 acc_val: 0.4878 time: 2.4482s\n",
      "Epoch: 0407 loss_train: 2.4917 acc_train: 0.5909 loss_val: 56.9706 acc_val: 0.4878 time: 2.5822s\n",
      "Epoch: 0408 loss_train: 2.4704 acc_train: 0.5927 loss_val: 54.7616 acc_val: 0.4878 time: 2.5422s\n",
      "Epoch: 0409 loss_train: 2.5217 acc_train: 0.5909 loss_val: 54.3776 acc_val: 0.5122 time: 2.4732s\n",
      "Epoch: 0410 loss_train: 2.4175 acc_train: 0.6014 loss_val: 53.2789 acc_val: 0.5000 time: 2.5202s\n",
      "Epoch: 0411 loss_train: 2.4419 acc_train: 0.5909 loss_val: 51.1461 acc_val: 0.5000 time: 2.6132s\n",
      "Epoch: 0412 loss_train: 2.4043 acc_train: 0.5997 loss_val: 50.3257 acc_val: 0.4878 time: 2.5612s\n",
      "Epoch: 0413 loss_train: 2.3990 acc_train: 0.5979 loss_val: 48.2564 acc_val: 0.5000 time: 2.3542s\n",
      "Epoch: 0414 loss_train: 2.3491 acc_train: 0.5962 loss_val: 47.4739 acc_val: 0.5000 time: 2.5382s\n",
      "Epoch: 0415 loss_train: 2.3936 acc_train: 0.5909 loss_val: 45.3956 acc_val: 0.5122 time: 2.4492s\n",
      "Epoch: 0416 loss_train: 2.3439 acc_train: 0.5892 loss_val: 42.7092 acc_val: 0.5122 time: 2.5792s\n",
      "Epoch: 0417 loss_train: 2.4774 acc_train: 0.5857 loss_val: 42.9018 acc_val: 0.4878 time: 2.4152s\n",
      "Epoch: 0418 loss_train: 2.3465 acc_train: 0.5979 loss_val: 41.4828 acc_val: 0.4878 time: 2.5302s\n",
      "Epoch: 0419 loss_train: 2.3500 acc_train: 0.5962 loss_val: 38.5141 acc_val: 0.5244 time: 2.8842s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0420 loss_train: 2.3841 acc_train: 0.6014 loss_val: 36.9275 acc_val: 0.5366 time: 2.7792s\n",
      "Epoch: 0421 loss_train: 2.3652 acc_train: 0.6066 loss_val: 36.6624 acc_val: 0.5122 time: 2.4652s\n",
      "Epoch: 0422 loss_train: 2.2974 acc_train: 0.6101 loss_val: 34.5044 acc_val: 0.5000 time: 2.4122s\n",
      "Epoch: 0423 loss_train: 2.2273 acc_train: 0.6084 loss_val: 33.0212 acc_val: 0.4878 time: 2.4942s\n",
      "Epoch: 0424 loss_train: 2.2136 acc_train: 0.6084 loss_val: 32.1325 acc_val: 0.5000 time: 2.3952s\n",
      "Epoch: 0425 loss_train: 2.2710 acc_train: 0.6154 loss_val: 29.2694 acc_val: 0.5488 time: 2.5102s\n",
      "Epoch: 0426 loss_train: 2.2782 acc_train: 0.6101 loss_val: 27.9029 acc_val: 0.5000 time: 2.5812s\n",
      "Epoch: 0427 loss_train: 2.2735 acc_train: 0.6084 loss_val: 27.7412 acc_val: 0.4878 time: 3.1342s\n",
      "Epoch: 0428 loss_train: 2.1691 acc_train: 0.6119 loss_val: 27.7285 acc_val: 0.4878 time: 2.7642s\n",
      "Epoch: 0429 loss_train: 2.3766 acc_train: 0.6136 loss_val: 25.3108 acc_val: 0.5122 time: 2.4582s\n",
      "Epoch: 0430 loss_train: 2.1515 acc_train: 0.6136 loss_val: 21.3475 acc_val: 0.5244 time: 2.4382s\n",
      "Epoch: 0431 loss_train: 2.4905 acc_train: 0.5909 loss_val: 20.9353 acc_val: 0.5000 time: 2.5062s\n",
      "Epoch: 0432 loss_train: 2.2410 acc_train: 0.6189 loss_val: 21.6000 acc_val: 0.5000 time: 2.5322s\n",
      "Epoch: 0433 loss_train: 2.2598 acc_train: 0.6119 loss_val: 19.7638 acc_val: 0.5122 time: 2.5762s\n",
      "Epoch: 0434 loss_train: 2.1638 acc_train: 0.6136 loss_val: 16.0292 acc_val: 0.5244 time: 2.6182s\n",
      "Epoch: 0435 loss_train: 2.3020 acc_train: 0.6154 loss_val: 14.7576 acc_val: 0.5366 time: 2.6632s\n",
      "Epoch: 0436 loss_train: 2.2561 acc_train: 0.6206 loss_val: 15.0854 acc_val: 0.5122 time: 2.8202s\n",
      "Epoch: 0437 loss_train: 2.1137 acc_train: 0.6119 loss_val: 15.8346 acc_val: 0.4878 time: 2.6462s\n",
      "Epoch: 0438 loss_train: 2.5937 acc_train: 0.6084 loss_val: 9.7120 acc_val: 0.5122 time: 2.5392s\n",
      "Epoch: 0439 loss_train: 2.9688 acc_train: 0.5892 loss_val: 10.2792 acc_val: 0.5366 time: 2.4202s\n",
      "Epoch: 0440 loss_train: 3.4371 acc_train: 0.5892 loss_val: 8.6812 acc_val: 0.5244 time: 2.5212s\n",
      "Epoch: 0441 loss_train: 2.3077 acc_train: 0.6189 loss_val: 12.5396 acc_val: 0.5000 time: 2.6302s\n",
      "Epoch: 0442 loss_train: 4.2196 acc_train: 0.6101 loss_val: 9.5770 acc_val: 0.5122 time: 2.7152s\n",
      "Epoch: 0443 loss_train: 2.8748 acc_train: 0.5839 loss_val: 11.1425 acc_val: 0.5000 time: 2.6372s\n",
      "Epoch: 0444 loss_train: 4.0625 acc_train: 0.5787 loss_val: 10.0385 acc_val: 0.5122 time: 2.6442s\n",
      "Epoch: 0445 loss_train: 3.1759 acc_train: 0.5944 loss_val: 6.9089 acc_val: 0.5244 time: 2.5142s\n",
      "Epoch: 0446 loss_train: 2.2150 acc_train: 0.6171 loss_val: 5.7364 acc_val: 0.5000 time: 2.4722s\n",
      "Epoch: 0447 loss_train: 4.0141 acc_train: 0.6031 loss_val: 11.2174 acc_val: 0.5366 time: 2.4152s\n",
      "Epoch: 0448 loss_train: 4.1342 acc_train: 0.5857 loss_val: 14.0307 acc_val: 0.5244 time: 2.4062s\n",
      "Epoch: 0449 loss_train: 6.3504 acc_train: 0.5822 loss_val: 13.7791 acc_val: 0.5244 time: 2.5442s\n",
      "Epoch: 0450 loss_train: 6.0717 acc_train: 0.5839 loss_val: 11.1317 acc_val: 0.5122 time: 2.6442s\n",
      "Epoch: 0451 loss_train: 4.0892 acc_train: 0.5927 loss_val: 6.7932 acc_val: 0.4756 time: 2.7162s\n",
      "Epoch: 0452 loss_train: 2.4525 acc_train: 0.6119 loss_val: 5.6078 acc_val: 0.5000 time: 2.5782s\n",
      "Epoch: 0453 loss_train: 4.2579 acc_train: 0.5979 loss_val: 11.4407 acc_val: 0.5244 time: 2.4742s\n",
      "Epoch: 0454 loss_train: 4.4184 acc_train: 0.5734 loss_val: 14.7668 acc_val: 0.5366 time: 2.4192s\n",
      "Epoch: 0455 loss_train: 6.6919 acc_train: 0.5839 loss_val: 15.0930 acc_val: 0.5366 time: 2.4292s\n",
      "Epoch: 0456 loss_train: 6.9503 acc_train: 0.5857 loss_val: 12.8453 acc_val: 0.5122 time: 2.4142s\n",
      "Epoch: 0457 loss_train: 5.2728 acc_train: 0.5944 loss_val: 8.9167 acc_val: 0.4878 time: 2.5152s\n",
      "Epoch: 0458 loss_train: 2.4783 acc_train: 0.6084 loss_val: 4.8687 acc_val: 0.4878 time: 2.8792s\n",
      "Epoch: 0459 loss_train: 7.7523 acc_train: 0.5874 loss_val: 8.5759 acc_val: 0.5000 time: 2.6872s\n",
      "Epoch: 0460 loss_train: 2.2863 acc_train: 0.6101 loss_val: 11.8555 acc_val: 0.5244 time: 2.4822s\n",
      "Epoch: 0461 loss_train: 4.6803 acc_train: 0.5839 loss_val: 12.1474 acc_val: 0.5610 time: 2.4332s\n",
      "Epoch: 0462 loss_train: 4.6883 acc_train: 0.6119 loss_val: 10.0025 acc_val: 0.5244 time: 2.6152s\n",
      "Epoch: 0463 loss_train: 2.9619 acc_train: 0.6101 loss_val: 6.3719 acc_val: 0.4512 time: 2.6172s\n",
      "Epoch: 0464 loss_train: 3.4531 acc_train: 0.6084 loss_val: 8.4673 acc_val: 0.4756 time: 2.6102s\n",
      "Epoch: 0465 loss_train: 2.4257 acc_train: 0.6136 loss_val: 9.7979 acc_val: 0.5244 time: 2.6792s\n",
      "Epoch: 0466 loss_train: 2.9120 acc_train: 0.5944 loss_val: 9.3605 acc_val: 0.5610 time: 2.6802s\n",
      "Epoch: 0467 loss_train: 2.6840 acc_train: 0.6276 loss_val: 7.4095 acc_val: 0.5488 time: 2.5572s\n",
      "Epoch: 0468 loss_train: 2.2217 acc_train: 0.6503 loss_val: 5.5058 acc_val: 0.5000 time: 2.4472s\n",
      "Epoch: 0469 loss_train: 4.2400 acc_train: 0.6049 loss_val: 11.0568 acc_val: 0.5000 time: 2.4252s\n",
      "Epoch: 0470 loss_train: 3.6212 acc_train: 0.5944 loss_val: 13.8640 acc_val: 0.4878 time: 2.6672s\n",
      "Epoch: 0471 loss_train: 5.8011 acc_train: 0.5944 loss_val: 13.5828 acc_val: 0.4756 time: 2.5452s\n",
      "Epoch: 0472 loss_train: 5.6084 acc_train: 0.6014 loss_val: 10.7836 acc_val: 0.5000 time: 2.7262s\n",
      "Epoch: 0473 loss_train: 3.5313 acc_train: 0.6119 loss_val: 6.3803 acc_val: 0.5000 time: 2.6742s\n",
      "Epoch: 0474 loss_train: 2.7364 acc_train: 0.6311 loss_val: 6.7009 acc_val: 0.5000 time: 2.6582s\n",
      "Epoch: 0475 loss_train: 2.3730 acc_train: 0.6346 loss_val: 8.8439 acc_val: 0.5000 time: 2.5612s\n",
      "Epoch: 0476 loss_train: 2.4991 acc_train: 0.6119 loss_val: 9.9356 acc_val: 0.5000 time: 2.5492s\n",
      "Epoch: 0477 loss_train: 2.9453 acc_train: 0.5979 loss_val: 8.9664 acc_val: 0.5122 time: 2.5082s\n",
      "Epoch: 0478 loss_train: 2.4169 acc_train: 0.6241 loss_val: 7.5318 acc_val: 0.5000 time: 2.4662s\n",
      "Epoch: 0479 loss_train: 2.1827 acc_train: 0.6276 loss_val: 6.4742 acc_val: 0.4756 time: 2.4972s\n",
      "Epoch: 0480 loss_train: 2.6261 acc_train: 0.6241 loss_val: 8.8410 acc_val: 0.5000 time: 2.5852s\n",
      "Epoch: 0481 loss_train: 2.3459 acc_train: 0.6259 loss_val: 10.1817 acc_val: 0.5122 time: 2.6392s\n",
      "Epoch: 0482 loss_train: 3.0860 acc_train: 0.6031 loss_val: 9.1705 acc_val: 0.5000 time: 2.5892s\n",
      "Epoch: 0483 loss_train: 2.4222 acc_train: 0.6276 loss_val: 7.1201 acc_val: 0.5000 time: 2.5392s\n",
      "Epoch: 0484 loss_train: 2.1200 acc_train: 0.6399 loss_val: 5.6892 acc_val: 0.5000 time: 2.4852s\n",
      "Epoch: 0485 loss_train: 3.5470 acc_train: 0.6294 loss_val: 10.0829 acc_val: 0.5122 time: 2.5262s\n",
      "Epoch: 0486 loss_train: 3.0501 acc_train: 0.6224 loss_val: 12.1066 acc_val: 0.4878 time: 2.5082s\n",
      "Epoch: 0487 loss_train: 4.5416 acc_train: 0.6031 loss_val: 11.6096 acc_val: 0.5000 time: 2.4042s\n",
      "Epoch: 0488 loss_train: 4.1792 acc_train: 0.6101 loss_val: 8.9531 acc_val: 0.5000 time: 2.8632s\n",
      "Epoch: 0489 loss_train: 2.3817 acc_train: 0.6329 loss_val: 5.3791 acc_val: 0.4878 time: 2.8812s\n",
      "Epoch: 0490 loss_train: 4.0859 acc_train: 0.6189 loss_val: 8.2016 acc_val: 0.5000 time: 2.4572s\n",
      "Epoch: 0491 loss_train: 2.1369 acc_train: 0.6259 loss_val: 10.3776 acc_val: 0.5122 time: 2.4142s\n",
      "Epoch: 0492 loss_train: 3.2388 acc_train: 0.6014 loss_val: 10.0382 acc_val: 0.5000 time: 2.5612s\n",
      "Epoch: 0493 loss_train: 2.9791 acc_train: 0.5997 loss_val: 7.5982 acc_val: 0.4878 time: 2.4962s\n"
     ]
    }
   ],
   "source": [
    "def gcn(features, adj, labels, idx_train, idx_val, idx_test):\n",
    "    lr, weight_decay, epochs = 0.01, 0, 2000\n",
    "    cuda = False\n",
    "    model = GCN(nfeat=features.shape[1],\n",
    "                nhid=8,\n",
    "                nclass=num_labels,\n",
    "                dropout=0.)\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=lr,\n",
    "                           weight_decay=weight_decay)\n",
    "\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "        features = features.cuda()\n",
    "        adj = adj.cuda()\n",
    "        labels = labels.cuda()\n",
    "        idx_train = idx_train.cuda()\n",
    "        idx_val = idx_val.cuda()\n",
    "        idx_test = idx_test.cuda()\n",
    "\n",
    "\n",
    "    def train(epoch):\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features, adj)\n",
    "        loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "        acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "        loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "              'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "        return loss_val\n",
    "\n",
    "\n",
    "    def test():\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "        loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "        acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "        print(\"Test set results:\",\n",
    "              \"loss= {:.4f}\".format(loss_test.item()),\n",
    "              \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "        with open(\"gcn.pkl\", \"wb\") as f:\n",
    "            pickle.dump(output.detach().numpy(), f)\n",
    "        return acc_test.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    # Train model\n",
    "    t_total = time.time()\n",
    "    val_loss_list = []\n",
    "    for epoch in range(epochs):\n",
    "        loss_val = train(epoch)\n",
    "#         val_loss_list.append(loss_val.detach().cpu().numpy())\n",
    "#         if len(val_loss_list) > 50 and np.mean(val_loss_list[-50:]) < loss_val:\n",
    "#             print(\"early stoping...\")\n",
    "#             break\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "    # Testing\n",
    "    return test()\n",
    "\n",
    "acc_test_list = []\n",
    "for i in range(1):\n",
    "    acc_test = gcn(features, adj, labels, idx_train, idx_val, idx_test)\n",
    "    acc_test_list.append(acc_test)\n",
    "print(\"result: {}\".format(np.mean(acc_test_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
