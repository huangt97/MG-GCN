{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "from tf.gat.models import SpGAT\n",
    "from tf.gat.utils import process\n",
    "from utils import *\n",
    "from load_ether import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpt_file = 'ether_pre_trained.ckpt'\n",
    "\n",
    "dataset = 'ether'\n",
    "\n",
    "# training params\n",
    "batch_size = 1\n",
    "nb_epochs = 100000\n",
    "patience = 100\n",
    "lr = 0.005  # learning rate\n",
    "l2_coef = 0.0005  # weight decay\n",
    "hid_units = [8] # numbers of hidden units per each attention head in each layer\n",
    "n_heads = [8, 1] # additional entry for the output layer\n",
    "residual = False\n",
    "nonlinearity = tf.nn.elu\n",
    "# model = GAT\n",
    "model = SpGAT\n",
    "sparse = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features = (1402220, 12) type = <class 'numpy.ndarray'>\n",
      "features = (1402220, 4) type = <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(\"./data/ether/\")\n",
    "\n",
    "features = features.toarray()\n",
    "print(\"features = {} type = {}\".format(features.shape, type(features)))\n",
    "in_transaction_value = np.array(features[:, 0])\n",
    "in_transaction_count = np.array(features[:, 1])\n",
    "out_transaction_value = np.array(features[:, 3])\n",
    "out_transaction_count = np.array(features[:, 4])\n",
    "in_transaction_value = np.reshape(in_transaction_value, newshape=(in_transaction_value.shape[0], 1))\n",
    "in_transaction_count = np.reshape(in_transaction_count, newshape=(in_transaction_count.shape[0], 1))\n",
    "out_transaction_value = np.reshape(out_transaction_value, newshape=(out_transaction_value.shape[0], 1))\n",
    "out_transaction_count = np.reshape(out_transaction_count, newshape=(out_transaction_count.shape[0], 1))\n",
    "features = np.concatenate((in_transaction_value, in_transaction_count, out_transaction_value, out_transaction_count), axis=1)\n",
    "print(\"features = {} type = {}\".format(features.shape, type(features)))\n",
    "\n",
    "features = sp.lil_matrix(features)\n",
    "features, spars = process.preprocess_features(features)\n",
    "\n",
    "nb_nodes = features.shape[0]\n",
    "ft_size = features.shape[1]\n",
    "nb_classes = y_train.shape[1]\n",
    "\n",
    "features = features[np.newaxis]\n",
    "y_train = y_train[np.newaxis]\n",
    "y_val = y_val[np.newaxis]\n",
    "y_test = y_test[np.newaxis]\n",
    "train_mask = train_mask[np.newaxis]\n",
    "val_mask = val_mask[np.newaxis]\n",
    "test_mask = test_mask[np.newaxis]\n",
    "biases = process.preprocess_adj_bias(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/zjt/MG-GCN/tf/gat/utils/layers.py:41: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ubuntu/zjt/MG-GCN/tf/gat/utils/layers.py:43: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv1D` instead.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ubuntu/zjt/MG-GCN/tf/gat/utils/layers.py:59: The name tf.sparse_softmax is deprecated. Please use tf.sparse.softmax instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/zjt/MG-GCN/tf/gat/utils/layers.py:71: The name tf.sparse_reshape is deprecated. Please use tf.sparse.reshape instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/zjt/MG-GCN/tf/gat/utils/layers.py:73: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/zjt/MG-GCN/tf/gat/models/base_gattn.py:41: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/zjt/MG-GCN/tf/gat/models/base_gattn.py:12: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/zjt/MG-GCN/tf/gat/models/base_gattn.py:17: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "Training: loss = 2.18025, acc = 0.22552 | Val: loss = 1.70874, acc = 0.17073\n",
      "Training: loss = 2.27202, acc = 0.18706 | Val: loss = 1.69309, acc = 0.17073\n",
      "Training: loss = 2.09797, acc = 0.18357 | Val: loss = 1.67654, acc = 0.17073\n",
      "Training: loss = 1.99001, acc = 0.25175 | Val: loss = 1.66070, acc = 0.24390\n",
      "Training: loss = 1.85638, acc = 0.23951 | Val: loss = 1.64492, acc = 0.29268\n",
      "Training: loss = 1.84554, acc = 0.25175 | Val: loss = 1.62990, acc = 0.31707\n",
      "Training: loss = 1.71285, acc = 0.32867 | Val: loss = 1.61444, acc = 0.30488\n",
      "Training: loss = 1.74132, acc = 0.27098 | Val: loss = 1.59893, acc = 0.31707\n",
      "Training: loss = 1.86998, acc = 0.22378 | Val: loss = 1.58376, acc = 0.32927\n",
      "Training: loss = 1.74124, acc = 0.26573 | Val: loss = 1.56852, acc = 0.32927\n",
      "Training: loss = 1.76278, acc = 0.27797 | Val: loss = 1.55366, acc = 0.32927\n",
      "Training: loss = 1.71128, acc = 0.31818 | Val: loss = 1.53939, acc = 0.32927\n",
      "Training: loss = 1.70027, acc = 0.31469 | Val: loss = 1.52580, acc = 0.36585\n",
      "Training: loss = 1.71828, acc = 0.31119 | Val: loss = 1.51286, acc = 0.42683\n",
      "Training: loss = 1.62786, acc = 0.30070 | Val: loss = 1.50055, acc = 0.47561\n",
      "Training: loss = 1.61170, acc = 0.33217 | Val: loss = 1.48923, acc = 0.51220\n",
      "Training: loss = 1.58973, acc = 0.34091 | Val: loss = 1.47832, acc = 0.53659\n",
      "Training: loss = 1.62970, acc = 0.32343 | Val: loss = 1.46796, acc = 0.52439\n",
      "Training: loss = 1.54054, acc = 0.38462 | Val: loss = 1.45797, acc = 0.52439\n",
      "Training: loss = 1.49698, acc = 0.38986 | Val: loss = 1.44838, acc = 0.50000\n",
      "Training: loss = 1.55891, acc = 0.31993 | Val: loss = 1.43931, acc = 0.51220\n",
      "Training: loss = 1.60294, acc = 0.33741 | Val: loss = 1.43097, acc = 0.51220\n",
      "Training: loss = 1.59863, acc = 0.31993 | Val: loss = 1.42320, acc = 0.51220\n",
      "Training: loss = 1.58305, acc = 0.34441 | Val: loss = 1.41589, acc = 0.50000\n",
      "Training: loss = 1.58841, acc = 0.36189 | Val: loss = 1.40910, acc = 0.50000\n",
      "Training: loss = 1.58240, acc = 0.32343 | Val: loss = 1.40236, acc = 0.50000\n",
      "Training: loss = 1.49407, acc = 0.37413 | Val: loss = 1.39599, acc = 0.50000\n",
      "Training: loss = 1.48858, acc = 0.35490 | Val: loss = 1.38997, acc = 0.50000\n",
      "Training: loss = 1.47482, acc = 0.40559 | Val: loss = 1.38410, acc = 0.50000\n",
      "Training: loss = 1.54469, acc = 0.33042 | Val: loss = 1.37842, acc = 0.50000\n",
      "Training: loss = 1.46747, acc = 0.39685 | Val: loss = 1.37310, acc = 0.50000\n",
      "Training: loss = 1.62850, acc = 0.36888 | Val: loss = 1.36796, acc = 0.50000\n",
      "Training: loss = 1.49534, acc = 0.37238 | Val: loss = 1.36303, acc = 0.50000\n",
      "Training: loss = 1.44587, acc = 0.37587 | Val: loss = 1.35794, acc = 0.50000\n",
      "Training: loss = 1.40922, acc = 0.39161 | Val: loss = 1.35323, acc = 0.50000\n",
      "Training: loss = 1.50068, acc = 0.37762 | Val: loss = 1.34881, acc = 0.51220\n",
      "Training: loss = 1.44725, acc = 0.37937 | Val: loss = 1.34445, acc = 0.51220\n",
      "Training: loss = 1.49949, acc = 0.37063 | Val: loss = 1.34033, acc = 0.51220\n",
      "Training: loss = 1.44773, acc = 0.38986 | Val: loss = 1.33646, acc = 0.51220\n",
      "Training: loss = 1.49005, acc = 0.39510 | Val: loss = 1.33271, acc = 0.51220\n",
      "Training: loss = 1.51178, acc = 0.34965 | Val: loss = 1.32911, acc = 0.51220\n",
      "Training: loss = 1.43323, acc = 0.39510 | Val: loss = 1.32581, acc = 0.51220\n",
      "Training: loss = 1.40930, acc = 0.44231 | Val: loss = 1.32269, acc = 0.51220\n",
      "Training: loss = 1.41356, acc = 0.41608 | Val: loss = 1.31982, acc = 0.51220\n",
      "Training: loss = 1.42649, acc = 0.41259 | Val: loss = 1.31704, acc = 0.52439\n",
      "Training: loss = 1.39122, acc = 0.44231 | Val: loss = 1.31443, acc = 0.52439\n",
      "Training: loss = 1.40162, acc = 0.44231 | Val: loss = 1.31191, acc = 0.51220\n",
      "Training: loss = 1.39524, acc = 0.45455 | Val: loss = 1.30936, acc = 0.51220\n",
      "Training: loss = 1.48761, acc = 0.43706 | Val: loss = 1.30705, acc = 0.51220\n",
      "Training: loss = 1.44577, acc = 0.42133 | Val: loss = 1.30478, acc = 0.51220\n",
      "Training: loss = 1.44529, acc = 0.41608 | Val: loss = 1.30269, acc = 0.51220\n",
      "Training: loss = 1.37408, acc = 0.44580 | Val: loss = 1.30065, acc = 0.51220\n",
      "Training: loss = 1.34626, acc = 0.45105 | Val: loss = 1.29868, acc = 0.51220\n",
      "Training: loss = 1.43180, acc = 0.44056 | Val: loss = 1.29661, acc = 0.51220\n",
      "Training: loss = 1.32789, acc = 0.46504 | Val: loss = 1.29471, acc = 0.51220\n",
      "Training: loss = 1.38623, acc = 0.41608 | Val: loss = 1.29306, acc = 0.51220\n",
      "Training: loss = 1.37317, acc = 0.46853 | Val: loss = 1.29160, acc = 0.51220\n",
      "Training: loss = 1.40576, acc = 0.42657 | Val: loss = 1.29014, acc = 0.51220\n",
      "Training: loss = 1.34277, acc = 0.47378 | Val: loss = 1.28902, acc = 0.51220\n",
      "Training: loss = 1.36930, acc = 0.44056 | Val: loss = 1.28823, acc = 0.51220\n",
      "Training: loss = 1.33146, acc = 0.46504 | Val: loss = 1.28770, acc = 0.51220\n",
      "Training: loss = 1.35299, acc = 0.47028 | Val: loss = 1.28743, acc = 0.51220\n",
      "Training: loss = 1.38373, acc = 0.43706 | Val: loss = 1.28753, acc = 0.52439\n",
      "Training: loss = 1.42566, acc = 0.46504 | Val: loss = 1.28790, acc = 0.52439\n",
      "Training: loss = 1.39714, acc = 0.45455 | Val: loss = 1.28863, acc = 0.51220\n",
      "Training: loss = 1.39189, acc = 0.44406 | Val: loss = 1.28900, acc = 0.51220\n",
      "Training: loss = 1.38667, acc = 0.42133 | Val: loss = 1.28903, acc = 0.51220\n",
      "Training: loss = 1.38596, acc = 0.44755 | Val: loss = 1.28873, acc = 0.52439\n",
      "Training: loss = 1.34589, acc = 0.47727 | Val: loss = 1.28828, acc = 0.52439\n",
      "Training: loss = 1.41601, acc = 0.42832 | Val: loss = 1.28743, acc = 0.51220\n",
      "Training: loss = 1.40041, acc = 0.43881 | Val: loss = 1.28691, acc = 0.51220\n",
      "Training: loss = 1.40778, acc = 0.42657 | Val: loss = 1.28612, acc = 0.51220\n",
      "Training: loss = 1.40949, acc = 0.43531 | Val: loss = 1.28520, acc = 0.51220\n",
      "Training: loss = 1.51282, acc = 0.38462 | Val: loss = 1.28427, acc = 0.51220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: loss = 1.36019, acc = 0.48427 | Val: loss = 1.28318, acc = 0.51220\n",
      "Training: loss = 1.29918, acc = 0.47203 | Val: loss = 1.28199, acc = 0.51220\n",
      "Training: loss = 1.35486, acc = 0.46329 | Val: loss = 1.28033, acc = 0.51220\n",
      "Training: loss = 1.35596, acc = 0.47552 | Val: loss = 1.27894, acc = 0.51220\n",
      "Training: loss = 1.34573, acc = 0.45804 | Val: loss = 1.27760, acc = 0.51220\n",
      "Training: loss = 1.38751, acc = 0.42832 | Val: loss = 1.27608, acc = 0.51220\n",
      "Training: loss = 1.38221, acc = 0.46678 | Val: loss = 1.27475, acc = 0.51220\n",
      "Training: loss = 1.33652, acc = 0.51049 | Val: loss = 1.27357, acc = 0.51220\n",
      "Training: loss = 1.40546, acc = 0.47552 | Val: loss = 1.27230, acc = 0.51220\n",
      "Training: loss = 1.33143, acc = 0.44930 | Val: loss = 1.27109, acc = 0.51220\n",
      "Training: loss = 1.28496, acc = 0.51748 | Val: loss = 1.26990, acc = 0.51220\n",
      "Training: loss = 1.33177, acc = 0.48252 | Val: loss = 1.26913, acc = 0.51220\n",
      "Training: loss = 1.35010, acc = 0.46329 | Val: loss = 1.26828, acc = 0.51220\n",
      "Training: loss = 1.39130, acc = 0.47378 | Val: loss = 1.26748, acc = 0.51220\n",
      "Training: loss = 1.33207, acc = 0.49476 | Val: loss = 1.26697, acc = 0.51220\n",
      "Training: loss = 1.40247, acc = 0.46329 | Val: loss = 1.26633, acc = 0.51220\n",
      "Training: loss = 1.44074, acc = 0.39336 | Val: loss = 1.26540, acc = 0.51220\n",
      "Training: loss = 1.36994, acc = 0.44231 | Val: loss = 1.26458, acc = 0.51220\n",
      "Training: loss = 1.31230, acc = 0.47727 | Val: loss = 1.26373, acc = 0.51220\n",
      "Training: loss = 1.26564, acc = 0.49301 | Val: loss = 1.26250, acc = 0.51220\n",
      "Training: loss = 1.25667, acc = 0.51049 | Val: loss = 1.26127, acc = 0.51220\n",
      "Training: loss = 1.33142, acc = 0.48776 | Val: loss = 1.25991, acc = 0.51220\n",
      "Training: loss = 1.27348, acc = 0.48776 | Val: loss = 1.25857, acc = 0.51220\n",
      "Training: loss = 1.28441, acc = 0.48427 | Val: loss = 1.25738, acc = 0.51220\n",
      "Training: loss = 1.31013, acc = 0.47203 | Val: loss = 1.25634, acc = 0.51220\n",
      "Training: loss = 1.28638, acc = 0.48427 | Val: loss = 1.25502, acc = 0.50000\n",
      "Training: loss = 1.37542, acc = 0.48252 | Val: loss = 1.25431, acc = 0.51220\n",
      "Training: loss = 1.30859, acc = 0.47378 | Val: loss = 1.25431, acc = 0.51220\n",
      "Training: loss = 1.34604, acc = 0.45280 | Val: loss = 1.25410, acc = 0.51220\n",
      "Training: loss = 1.28910, acc = 0.47902 | Val: loss = 1.25389, acc = 0.51220\n",
      "Training: loss = 1.33275, acc = 0.47028 | Val: loss = 1.25352, acc = 0.51220\n",
      "Training: loss = 1.36290, acc = 0.44406 | Val: loss = 1.25323, acc = 0.51220\n",
      "Training: loss = 1.35336, acc = 0.45804 | Val: loss = 1.25280, acc = 0.51220\n",
      "Training: loss = 1.35595, acc = 0.48776 | Val: loss = 1.25226, acc = 0.51220\n",
      "Training: loss = 1.35440, acc = 0.45280 | Val: loss = 1.25159, acc = 0.51220\n",
      "Training: loss = 1.33755, acc = 0.47902 | Val: loss = 1.25122, acc = 0.51220\n",
      "Training: loss = 1.34337, acc = 0.48601 | Val: loss = 1.25058, acc = 0.51220\n",
      "Training: loss = 1.27811, acc = 0.52797 | Val: loss = 1.25014, acc = 0.51220\n",
      "Training: loss = 1.27273, acc = 0.49650 | Val: loss = 1.24990, acc = 0.51220\n",
      "Training: loss = 1.27829, acc = 0.50000 | Val: loss = 1.25000, acc = 0.51220\n",
      "Training: loss = 1.33421, acc = 0.45979 | Val: loss = 1.25037, acc = 0.51220\n",
      "Training: loss = 1.31640, acc = 0.49476 | Val: loss = 1.25096, acc = 0.51220\n",
      "Training: loss = 1.34718, acc = 0.48776 | Val: loss = 1.25159, acc = 0.51220\n",
      "Training: loss = 1.35821, acc = 0.46329 | Val: loss = 1.25219, acc = 0.51220\n",
      "Training: loss = 1.38767, acc = 0.44580 | Val: loss = 1.25343, acc = 0.51220\n",
      "Training: loss = 1.26076, acc = 0.50175 | Val: loss = 1.25462, acc = 0.51220\n",
      "Training: loss = 1.33885, acc = 0.50699 | Val: loss = 1.25618, acc = 0.51220\n",
      "Training: loss = 1.38831, acc = 0.46154 | Val: loss = 1.25735, acc = 0.51220\n",
      "Training: loss = 1.32655, acc = 0.48601 | Val: loss = 1.25820, acc = 0.51220\n",
      "Training: loss = 1.29390, acc = 0.52972 | Val: loss = 1.25840, acc = 0.51220\n",
      "Training: loss = 1.31902, acc = 0.48252 | Val: loss = 1.25857, acc = 0.51220\n",
      "Training: loss = 1.39411, acc = 0.49650 | Val: loss = 1.25883, acc = 0.51220\n",
      "Training: loss = 1.26757, acc = 0.48951 | Val: loss = 1.25920, acc = 0.51220\n",
      "Training: loss = 1.29105, acc = 0.48252 | Val: loss = 1.25888, acc = 0.51220\n",
      "Training: loss = 1.27352, acc = 0.52622 | Val: loss = 1.25874, acc = 0.51220\n",
      "Training: loss = 1.31185, acc = 0.50000 | Val: loss = 1.25886, acc = 0.51220\n",
      "Training: loss = 1.27524, acc = 0.48951 | Val: loss = 1.25885, acc = 0.51220\n",
      "Training: loss = 1.32523, acc = 0.47028 | Val: loss = 1.25786, acc = 0.51220\n",
      "Training: loss = 1.31640, acc = 0.52098 | Val: loss = 1.25695, acc = 0.51220\n",
      "Training: loss = 1.27302, acc = 0.50524 | Val: loss = 1.25621, acc = 0.51220\n",
      "Training: loss = 1.39423, acc = 0.44580 | Val: loss = 1.25531, acc = 0.51220\n",
      "Training: loss = 1.37415, acc = 0.46678 | Val: loss = 1.25413, acc = 0.51220\n",
      "Training: loss = 1.30326, acc = 0.51049 | Val: loss = 1.25303, acc = 0.51220\n",
      "Training: loss = 1.34243, acc = 0.49650 | Val: loss = 1.25224, acc = 0.51220\n",
      "Training: loss = 1.32338, acc = 0.50000 | Val: loss = 1.25127, acc = 0.51220\n",
      "Training: loss = 1.28486, acc = 0.50699 | Val: loss = 1.25017, acc = 0.51220\n",
      "Training: loss = 1.27249, acc = 0.54371 | Val: loss = 1.24946, acc = 0.51220\n",
      "Training: loss = 1.29797, acc = 0.50000 | Val: loss = 1.24872, acc = 0.51220\n",
      "Training: loss = 1.34685, acc = 0.49126 | Val: loss = 1.24809, acc = 0.51220\n",
      "Training: loss = 1.28023, acc = 0.51049 | Val: loss = 1.24750, acc = 0.51220\n",
      "Training: loss = 1.34469, acc = 0.49650 | Val: loss = 1.24746, acc = 0.51220\n",
      "Training: loss = 1.27692, acc = 0.49301 | Val: loss = 1.24737, acc = 0.51220\n",
      "Training: loss = 1.28768, acc = 0.49301 | Val: loss = 1.24765, acc = 0.51220\n",
      "Training: loss = 1.23317, acc = 0.50699 | Val: loss = 1.24759, acc = 0.51220\n",
      "Training: loss = 1.29338, acc = 0.50000 | Val: loss = 1.24736, acc = 0.51220\n",
      "Training: loss = 1.26009, acc = 0.50350 | Val: loss = 1.24723, acc = 0.51220\n",
      "Training: loss = 1.29251, acc = 0.50175 | Val: loss = 1.24739, acc = 0.51220\n",
      "Training: loss = 1.31829, acc = 0.45804 | Val: loss = 1.24728, acc = 0.51220\n",
      "Training: loss = 1.29838, acc = 0.47727 | Val: loss = 1.24738, acc = 0.51220\n",
      "Training: loss = 1.27669, acc = 0.53322 | Val: loss = 1.24699, acc = 0.51220\n",
      "Training: loss = 1.21193, acc = 0.51049 | Val: loss = 1.24616, acc = 0.51220\n",
      "Training: loss = 1.29765, acc = 0.50175 | Val: loss = 1.24562, acc = 0.51220\n",
      "Training: loss = 1.20845, acc = 0.53671 | Val: loss = 1.24549, acc = 0.51220\n",
      "Training: loss = 1.28589, acc = 0.49301 | Val: loss = 1.24526, acc = 0.51220\n",
      "Training: loss = 1.21928, acc = 0.52448 | Val: loss = 1.24472, acc = 0.51220\n",
      "Training: loss = 1.22990, acc = 0.49476 | Val: loss = 1.24414, acc = 0.51220\n",
      "Training: loss = 1.20596, acc = 0.54720 | Val: loss = 1.24319, acc = 0.51220\n",
      "Training: loss = 1.28256, acc = 0.51748 | Val: loss = 1.24154, acc = 0.51220\n",
      "Training: loss = 1.33567, acc = 0.47552 | Val: loss = 1.24012, acc = 0.51220\n",
      "Training: loss = 1.25319, acc = 0.51399 | Val: loss = 1.23883, acc = 0.51220\n",
      "Training: loss = 1.28443, acc = 0.52273 | Val: loss = 1.23767, acc = 0.51220\n",
      "Training: loss = 1.33184, acc = 0.48427 | Val: loss = 1.23699, acc = 0.51220\n",
      "Training: loss = 1.26622, acc = 0.51049 | Val: loss = 1.23685, acc = 0.51220\n",
      "Training: loss = 1.26681, acc = 0.51573 | Val: loss = 1.23688, acc = 0.51220\n",
      "Training: loss = 1.23939, acc = 0.51748 | Val: loss = 1.23709, acc = 0.51220\n",
      "Training: loss = 1.32563, acc = 0.51224 | Val: loss = 1.23753, acc = 0.51220\n",
      "Training: loss = 1.28025, acc = 0.52098 | Val: loss = 1.23802, acc = 0.51220\n",
      "Training: loss = 1.33842, acc = 0.49126 | Val: loss = 1.23867, acc = 0.51220\n",
      "Training: loss = 1.33698, acc = 0.47378 | Val: loss = 1.23866, acc = 0.51220\n",
      "Training: loss = 1.29806, acc = 0.51049 | Val: loss = 1.23876, acc = 0.51220\n",
      "Training: loss = 1.33187, acc = 0.47727 | Val: loss = 1.23882, acc = 0.51220\n",
      "Training: loss = 1.30939, acc = 0.50000 | Val: loss = 1.23832, acc = 0.51220\n",
      "Training: loss = 1.25752, acc = 0.52448 | Val: loss = 1.23790, acc = 0.51220\n",
      "Training: loss = 1.36885, acc = 0.46329 | Val: loss = 1.23756, acc = 0.51220\n",
      "Training: loss = 1.29292, acc = 0.52622 | Val: loss = 1.23678, acc = 0.51220\n",
      "Training: loss = 1.32182, acc = 0.47727 | Val: loss = 1.23574, acc = 0.51220\n",
      "Training: loss = 1.25201, acc = 0.50874 | Val: loss = 1.23420, acc = 0.51220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: loss = 1.27102, acc = 0.49301 | Val: loss = 1.23361, acc = 0.51220\n",
      "Training: loss = 1.28897, acc = 0.48427 | Val: loss = 1.23323, acc = 0.51220\n",
      "Training: loss = 1.30172, acc = 0.50350 | Val: loss = 1.23383, acc = 0.51220\n",
      "Training: loss = 1.33319, acc = 0.45979 | Val: loss = 1.23434, acc = 0.51220\n",
      "Training: loss = 1.25076, acc = 0.50699 | Val: loss = 1.23487, acc = 0.51220\n",
      "Training: loss = 1.23641, acc = 0.55594 | Val: loss = 1.23531, acc = 0.51220\n",
      "Training: loss = 1.30092, acc = 0.49301 | Val: loss = 1.23570, acc = 0.51220\n",
      "Training: loss = 1.22239, acc = 0.50874 | Val: loss = 1.23572, acc = 0.51220\n",
      "Training: loss = 1.33194, acc = 0.51923 | Val: loss = 1.23609, acc = 0.51220\n",
      "Training: loss = 1.27570, acc = 0.50350 | Val: loss = 1.23625, acc = 0.51220\n",
      "Training: loss = 1.28992, acc = 0.48601 | Val: loss = 1.23631, acc = 0.51220\n",
      "Training: loss = 1.27580, acc = 0.55070 | Val: loss = 1.23672, acc = 0.51220\n",
      "Training: loss = 1.22522, acc = 0.52273 | Val: loss = 1.23719, acc = 0.51220\n",
      "Training: loss = 1.24334, acc = 0.50000 | Val: loss = 1.23739, acc = 0.51220\n",
      "Training: loss = 1.37304, acc = 0.48427 | Val: loss = 1.23838, acc = 0.51220\n",
      "Training: loss = 1.26308, acc = 0.51399 | Val: loss = 1.23910, acc = 0.51220\n",
      "Training: loss = 1.32214, acc = 0.49650 | Val: loss = 1.23978, acc = 0.51220\n",
      "Training: loss = 1.36347, acc = 0.44755 | Val: loss = 1.24105, acc = 0.51220\n",
      "Training: loss = 1.19611, acc = 0.55245 | Val: loss = 1.24243, acc = 0.51220\n",
      "Training: loss = 1.27595, acc = 0.51224 | Val: loss = 1.24332, acc = 0.51220\n",
      "Training: loss = 1.27490, acc = 0.50000 | Val: loss = 1.24423, acc = 0.51220\n",
      "Training: loss = 1.31067, acc = 0.48776 | Val: loss = 1.24503, acc = 0.51220\n",
      "Training: loss = 1.29572, acc = 0.48077 | Val: loss = 1.24537, acc = 0.51220\n",
      "Training: loss = 1.29799, acc = 0.53147 | Val: loss = 1.24581, acc = 0.51220\n",
      "Training: loss = 1.29814, acc = 0.49650 | Val: loss = 1.24563, acc = 0.51220\n",
      "Training: loss = 1.27580, acc = 0.49825 | Val: loss = 1.24463, acc = 0.51220\n",
      "Training: loss = 1.31457, acc = 0.50699 | Val: loss = 1.24413, acc = 0.51220\n",
      "Training: loss = 1.26302, acc = 0.50524 | Val: loss = 1.24305, acc = 0.51220\n",
      "Training: loss = 1.22462, acc = 0.53147 | Val: loss = 1.24239, acc = 0.51220\n",
      "Training: loss = 1.27068, acc = 0.52797 | Val: loss = 1.24105, acc = 0.51220\n",
      "Training: loss = 1.25050, acc = 0.51049 | Val: loss = 1.23954, acc = 0.51220\n",
      "Training: loss = 1.24165, acc = 0.50175 | Val: loss = 1.23750, acc = 0.51220\n",
      "Training: loss = 1.28512, acc = 0.49476 | Val: loss = 1.23494, acc = 0.51220\n",
      "Training: loss = 1.36741, acc = 0.46853 | Val: loss = 1.23293, acc = 0.51220\n",
      "Training: loss = 1.24361, acc = 0.54720 | Val: loss = 1.23108, acc = 0.52439\n",
      "Training: loss = 1.28721, acc = 0.48601 | Val: loss = 1.23044, acc = 0.51220\n",
      "Training: loss = 1.25326, acc = 0.47727 | Val: loss = 1.23036, acc = 0.51220\n",
      "Training: loss = 1.26898, acc = 0.53671 | Val: loss = 1.23077, acc = 0.51220\n",
      "Training: loss = 1.24258, acc = 0.48601 | Val: loss = 1.23115, acc = 0.51220\n",
      "Training: loss = 1.27425, acc = 0.51224 | Val: loss = 1.23164, acc = 0.51220\n",
      "Training: loss = 1.33202, acc = 0.49650 | Val: loss = 1.23218, acc = 0.51220\n",
      "Training: loss = 1.26740, acc = 0.50175 | Val: loss = 1.23286, acc = 0.51220\n",
      "Training: loss = 1.21568, acc = 0.51748 | Val: loss = 1.23413, acc = 0.52439\n",
      "Training: loss = 1.19932, acc = 0.52622 | Val: loss = 1.23503, acc = 0.51220\n",
      "Training: loss = 1.32338, acc = 0.51049 | Val: loss = 1.23627, acc = 0.51220\n",
      "Training: loss = 1.27126, acc = 0.48951 | Val: loss = 1.23716, acc = 0.51220\n",
      "Training: loss = 1.25649, acc = 0.48601 | Val: loss = 1.23797, acc = 0.51220\n",
      "Training: loss = 1.28932, acc = 0.48776 | Val: loss = 1.23830, acc = 0.51220\n",
      "Training: loss = 1.28990, acc = 0.50350 | Val: loss = 1.23833, acc = 0.51220\n",
      "Training: loss = 1.27922, acc = 0.50000 | Val: loss = 1.23797, acc = 0.51220\n",
      "Training: loss = 1.23989, acc = 0.53322 | Val: loss = 1.23766, acc = 0.51220\n",
      "Training: loss = 1.24487, acc = 0.51049 | Val: loss = 1.23706, acc = 0.51220\n",
      "Training: loss = 1.14874, acc = 0.50874 | Val: loss = 1.23601, acc = 0.51220\n",
      "Training: loss = 1.20340, acc = 0.53497 | Val: loss = 1.23517, acc = 0.51220\n",
      "Training: loss = 1.30198, acc = 0.51224 | Val: loss = 1.23396, acc = 0.51220\n",
      "Training: loss = 1.33886, acc = 0.47727 | Val: loss = 1.23157, acc = 0.51220\n",
      "Training: loss = 1.27613, acc = 0.50000 | Val: loss = 1.22914, acc = 0.51220\n",
      "Training: loss = 1.21644, acc = 0.50874 | Val: loss = 1.22708, acc = 0.51220\n",
      "Training: loss = 1.36976, acc = 0.46504 | Val: loss = 1.22610, acc = 0.51220\n",
      "Training: loss = 1.29476, acc = 0.50000 | Val: loss = 1.22541, acc = 0.51220\n",
      "Training: loss = 1.23792, acc = 0.52448 | Val: loss = 1.22490, acc = 0.51220\n",
      "Training: loss = 1.23452, acc = 0.48601 | Val: loss = 1.22418, acc = 0.51220\n",
      "Training: loss = 1.17971, acc = 0.56294 | Val: loss = 1.22352, acc = 0.51220\n",
      "Training: loss = 1.29145, acc = 0.50350 | Val: loss = 1.22253, acc = 0.51220\n",
      "Training: loss = 1.33555, acc = 0.51573 | Val: loss = 1.22185, acc = 0.51220\n",
      "Training: loss = 1.29448, acc = 0.48951 | Val: loss = 1.22138, acc = 0.51220\n",
      "Training: loss = 1.26761, acc = 0.51923 | Val: loss = 1.22076, acc = 0.51220\n",
      "Training: loss = 1.22201, acc = 0.54196 | Val: loss = 1.22074, acc = 0.51220\n",
      "Training: loss = 1.24132, acc = 0.53671 | Val: loss = 1.22060, acc = 0.51220\n",
      "Training: loss = 1.25477, acc = 0.52972 | Val: loss = 1.22092, acc = 0.51220\n",
      "Training: loss = 1.21046, acc = 0.53322 | Val: loss = 1.22187, acc = 0.51220\n",
      "Training: loss = 1.21569, acc = 0.52797 | Val: loss = 1.22260, acc = 0.51220\n",
      "Training: loss = 1.23653, acc = 0.51923 | Val: loss = 1.22352, acc = 0.51220\n",
      "Training: loss = 1.23282, acc = 0.54196 | Val: loss = 1.22425, acc = 0.51220\n",
      "Training: loss = 1.22106, acc = 0.49650 | Val: loss = 1.22531, acc = 0.51220\n",
      "Training: loss = 1.20022, acc = 0.53497 | Val: loss = 1.22697, acc = 0.51220\n",
      "Training: loss = 1.29236, acc = 0.47203 | Val: loss = 1.22882, acc = 0.51220\n",
      "Training: loss = 1.27003, acc = 0.52972 | Val: loss = 1.23086, acc = 0.51220\n",
      "Training: loss = 1.26138, acc = 0.51399 | Val: loss = 1.23228, acc = 0.51220\n",
      "Training: loss = 1.31392, acc = 0.47552 | Val: loss = 1.23343, acc = 0.51220\n",
      "Training: loss = 1.24142, acc = 0.50524 | Val: loss = 1.23387, acc = 0.51220\n",
      "Training: loss = 1.30073, acc = 0.47552 | Val: loss = 1.23413, acc = 0.51220\n",
      "Training: loss = 1.23930, acc = 0.52098 | Val: loss = 1.23418, acc = 0.51220\n",
      "Training: loss = 1.29514, acc = 0.48427 | Val: loss = 1.23347, acc = 0.51220\n",
      "Training: loss = 1.26876, acc = 0.49650 | Val: loss = 1.23200, acc = 0.51220\n",
      "Training: loss = 1.25141, acc = 0.51573 | Val: loss = 1.23061, acc = 0.51220\n",
      "Training: loss = 1.32588, acc = 0.48077 | Val: loss = 1.22900, acc = 0.51220\n",
      "Training: loss = 1.26817, acc = 0.50175 | Val: loss = 1.22751, acc = 0.51220\n",
      "Training: loss = 1.23211, acc = 0.54021 | Val: loss = 1.22603, acc = 0.50000\n",
      "Training: loss = 1.27981, acc = 0.50175 | Val: loss = 1.22540, acc = 0.50000\n",
      "Training: loss = 1.28820, acc = 0.49825 | Val: loss = 1.22490, acc = 0.50000\n",
      "Training: loss = 1.30500, acc = 0.48252 | Val: loss = 1.22463, acc = 0.50000\n",
      "Training: loss = 1.23622, acc = 0.52098 | Val: loss = 1.22444, acc = 0.50000\n",
      "Training: loss = 1.27027, acc = 0.53322 | Val: loss = 1.22434, acc = 0.50000\n",
      "Training: loss = 1.24669, acc = 0.52448 | Val: loss = 1.22435, acc = 0.50000\n",
      "Training: loss = 1.26573, acc = 0.52448 | Val: loss = 1.22380, acc = 0.51220\n",
      "Training: loss = 1.26343, acc = 0.51049 | Val: loss = 1.22316, acc = 0.51220\n",
      "Training: loss = 1.36326, acc = 0.50000 | Val: loss = 1.22301, acc = 0.51220\n",
      "Training: loss = 1.18649, acc = 0.52448 | Val: loss = 1.22316, acc = 0.51220\n",
      "Training: loss = 1.23647, acc = 0.54196 | Val: loss = 1.22273, acc = 0.51220\n",
      "Training: loss = 1.22523, acc = 0.53497 | Val: loss = 1.22256, acc = 0.51220\n",
      "Training: loss = 1.27800, acc = 0.51573 | Val: loss = 1.22256, acc = 0.51220\n",
      "Training: loss = 1.24136, acc = 0.52098 | Val: loss = 1.22311, acc = 0.51220\n",
      "Training: loss = 1.26260, acc = 0.50699 | Val: loss = 1.22379, acc = 0.51220\n",
      "Training: loss = 1.26439, acc = 0.49301 | Val: loss = 1.22426, acc = 0.51220\n",
      "Training: loss = 1.22203, acc = 0.54196 | Val: loss = 1.22402, acc = 0.51220\n",
      "Training: loss = 1.24819, acc = 0.50175 | Val: loss = 1.22344, acc = 0.51220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: loss = 1.20078, acc = 0.54021 | Val: loss = 1.22279, acc = 0.51220\n",
      "Training: loss = 1.23055, acc = 0.51399 | Val: loss = 1.22225, acc = 0.51220\n",
      "Training: loss = 1.20469, acc = 0.51573 | Val: loss = 1.22139, acc = 0.51220\n",
      "Training: loss = 1.26080, acc = 0.48252 | Val: loss = 1.22068, acc = 0.51220\n",
      "Training: loss = 1.30696, acc = 0.47552 | Val: loss = 1.22095, acc = 0.51220\n",
      "Training: loss = 1.23937, acc = 0.51049 | Val: loss = 1.22147, acc = 0.51220\n",
      "Training: loss = 1.30222, acc = 0.48427 | Val: loss = 1.22139, acc = 0.51220\n",
      "Training: loss = 1.22179, acc = 0.51224 | Val: loss = 1.22143, acc = 0.51220\n",
      "Training: loss = 1.29335, acc = 0.51049 | Val: loss = 1.22079, acc = 0.51220\n",
      "Training: loss = 1.26199, acc = 0.49825 | Val: loss = 1.22066, acc = 0.51220\n",
      "Training: loss = 1.24816, acc = 0.50350 | Val: loss = 1.22032, acc = 0.51220\n",
      "Training: loss = 1.27510, acc = 0.49301 | Val: loss = 1.22032, acc = 0.51220\n",
      "Training: loss = 1.21877, acc = 0.51399 | Val: loss = 1.22116, acc = 0.51220\n",
      "Training: loss = 1.27943, acc = 0.52448 | Val: loss = 1.22212, acc = 0.51220\n",
      "Training: loss = 1.24627, acc = 0.47028 | Val: loss = 1.22344, acc = 0.51220\n",
      "Training: loss = 1.28633, acc = 0.50699 | Val: loss = 1.22541, acc = 0.51220\n",
      "Training: loss = 1.22644, acc = 0.53322 | Val: loss = 1.22739, acc = 0.51220\n",
      "Training: loss = 1.26396, acc = 0.51573 | Val: loss = 1.22913, acc = 0.51220\n",
      "Training: loss = 1.28221, acc = 0.48951 | Val: loss = 1.23022, acc = 0.51220\n",
      "Training: loss = 1.26521, acc = 0.51224 | Val: loss = 1.23011, acc = 0.51220\n",
      "Training: loss = 1.21863, acc = 0.50350 | Val: loss = 1.22975, acc = 0.51220\n",
      "Training: loss = 1.24550, acc = 0.50000 | Val: loss = 1.22879, acc = 0.51220\n",
      "Training: loss = 1.25942, acc = 0.54895 | Val: loss = 1.22719, acc = 0.50000\n",
      "Training: loss = 1.23039, acc = 0.51748 | Val: loss = 1.22614, acc = 0.50000\n",
      "Training: loss = 1.24308, acc = 0.51049 | Val: loss = 1.22482, acc = 0.50000\n",
      "Training: loss = 1.23546, acc = 0.53497 | Val: loss = 1.22446, acc = 0.50000\n",
      "Training: loss = 1.27354, acc = 0.53497 | Val: loss = 1.22420, acc = 0.51220\n",
      "Training: loss = 1.21674, acc = 0.53322 | Val: loss = 1.22454, acc = 0.50000\n",
      "Training: loss = 1.17693, acc = 0.51573 | Val: loss = 1.22512, acc = 0.50000\n",
      "Training: loss = 1.29521, acc = 0.50350 | Val: loss = 1.22555, acc = 0.50000\n",
      "Training: loss = 1.24601, acc = 0.49825 | Val: loss = 1.22657, acc = 0.50000\n",
      "Training: loss = 1.25697, acc = 0.52622 | Val: loss = 1.22790, acc = 0.51220\n",
      "Training: loss = 1.21997, acc = 0.53147 | Val: loss = 1.22871, acc = 0.51220\n",
      "Training: loss = 1.21034, acc = 0.54196 | Val: loss = 1.22935, acc = 0.51220\n",
      "Training: loss = 1.27058, acc = 0.50874 | Val: loss = 1.22893, acc = 0.51220\n",
      "Training: loss = 1.28583, acc = 0.50524 | Val: loss = 1.22861, acc = 0.51220\n",
      "Training: loss = 1.21465, acc = 0.52098 | Val: loss = 1.22774, acc = 0.51220\n",
      "Training: loss = 1.30539, acc = 0.50175 | Val: loss = 1.22726, acc = 0.51220\n",
      "Training: loss = 1.21118, acc = 0.52273 | Val: loss = 1.22562, acc = 0.51220\n",
      "Training: loss = 1.26812, acc = 0.48252 | Val: loss = 1.22335, acc = 0.51220\n",
      "Training: loss = 1.25073, acc = 0.52273 | Val: loss = 1.22071, acc = 0.51220\n",
      "Training: loss = 1.25855, acc = 0.50000 | Val: loss = 1.21792, acc = 0.51220\n",
      "Training: loss = 1.28019, acc = 0.49126 | Val: loss = 1.21577, acc = 0.51220\n",
      "Training: loss = 1.20161, acc = 0.52797 | Val: loss = 1.21384, acc = 0.51220\n",
      "Training: loss = 1.24237, acc = 0.51923 | Val: loss = 1.21201, acc = 0.51220\n",
      "Training: loss = 1.26017, acc = 0.50524 | Val: loss = 1.21056, acc = 0.51220\n",
      "Training: loss = 1.21647, acc = 0.52972 | Val: loss = 1.20983, acc = 0.51220\n",
      "Training: loss = 1.24769, acc = 0.52098 | Val: loss = 1.21025, acc = 0.51220\n",
      "Training: loss = 1.27052, acc = 0.50350 | Val: loss = 1.21045, acc = 0.51220\n",
      "Training: loss = 1.26006, acc = 0.52622 | Val: loss = 1.21099, acc = 0.50000\n",
      "Training: loss = 1.29095, acc = 0.50699 | Val: loss = 1.21181, acc = 0.50000\n",
      "Training: loss = 1.28172, acc = 0.50350 | Val: loss = 1.21357, acc = 0.50000\n",
      "Training: loss = 1.16541, acc = 0.54720 | Val: loss = 1.21484, acc = 0.50000\n",
      "Training: loss = 1.19202, acc = 0.52972 | Val: loss = 1.21592, acc = 0.50000\n",
      "Training: loss = 1.20357, acc = 0.54196 | Val: loss = 1.21646, acc = 0.50000\n",
      "Training: loss = 1.20839, acc = 0.50524 | Val: loss = 1.21673, acc = 0.50000\n",
      "Training: loss = 1.14745, acc = 0.54371 | Val: loss = 1.21627, acc = 0.51220\n",
      "Training: loss = 1.24699, acc = 0.50350 | Val: loss = 1.21579, acc = 0.51220\n",
      "Training: loss = 1.24060, acc = 0.51399 | Val: loss = 1.21480, acc = 0.51220\n",
      "Training: loss = 1.25523, acc = 0.53671 | Val: loss = 1.21368, acc = 0.51220\n",
      "Training: loss = 1.19533, acc = 0.52448 | Val: loss = 1.21363, acc = 0.51220\n",
      "Training: loss = 1.15515, acc = 0.55245 | Val: loss = 1.21358, acc = 0.51220\n",
      "Training: loss = 1.22211, acc = 0.54021 | Val: loss = 1.21373, acc = 0.51220\n",
      "Training: loss = 1.18672, acc = 0.50350 | Val: loss = 1.21405, acc = 0.51220\n",
      "Training: loss = 1.25543, acc = 0.49825 | Val: loss = 1.21378, acc = 0.51220\n",
      "Training: loss = 1.19657, acc = 0.53671 | Val: loss = 1.21365, acc = 0.51220\n",
      "Training: loss = 1.24399, acc = 0.52622 | Val: loss = 1.21364, acc = 0.51220\n",
      "Training: loss = 1.18556, acc = 0.54196 | Val: loss = 1.21278, acc = 0.51220\n",
      "Training: loss = 1.23790, acc = 0.53846 | Val: loss = 1.21226, acc = 0.51220\n",
      "Training: loss = 1.32733, acc = 0.46678 | Val: loss = 1.21190, acc = 0.51220\n",
      "Training: loss = 1.22818, acc = 0.51399 | Val: loss = 1.21219, acc = 0.51220\n",
      "Training: loss = 1.19929, acc = 0.58042 | Val: loss = 1.21292, acc = 0.51220\n",
      "Training: loss = 1.30590, acc = 0.49126 | Val: loss = 1.21361, acc = 0.51220\n",
      "Training: loss = 1.22616, acc = 0.54720 | Val: loss = 1.21456, acc = 0.51220\n",
      "Training: loss = 1.19444, acc = 0.55070 | Val: loss = 1.21551, acc = 0.51220\n",
      "Training: loss = 1.16054, acc = 0.56119 | Val: loss = 1.21648, acc = 0.51220\n",
      "Training: loss = 1.27355, acc = 0.52448 | Val: loss = 1.21792, acc = 0.51220\n",
      "Training: loss = 1.29114, acc = 0.51573 | Val: loss = 1.21828, acc = 0.50000\n",
      "Training: loss = 1.20514, acc = 0.53846 | Val: loss = 1.21841, acc = 0.50000\n",
      "Training: loss = 1.23239, acc = 0.52273 | Val: loss = 1.21844, acc = 0.50000\n",
      "Training: loss = 1.19565, acc = 0.54895 | Val: loss = 1.21791, acc = 0.50000\n",
      "Training: loss = 1.27666, acc = 0.51224 | Val: loss = 1.21778, acc = 0.50000\n",
      "Training: loss = 1.25200, acc = 0.49476 | Val: loss = 1.21783, acc = 0.50000\n",
      "Training: loss = 1.23686, acc = 0.53322 | Val: loss = 1.21799, acc = 0.50000\n",
      "Training: loss = 1.23221, acc = 0.53497 | Val: loss = 1.21717, acc = 0.51220\n",
      "Training: loss = 1.21666, acc = 0.55070 | Val: loss = 1.21642, acc = 0.51220\n",
      "Training: loss = 1.26268, acc = 0.51748 | Val: loss = 1.21522, acc = 0.51220\n",
      "Training: loss = 1.30304, acc = 0.49650 | Val: loss = 1.21431, acc = 0.51220\n",
      "Training: loss = 1.23108, acc = 0.52972 | Val: loss = 1.21289, acc = 0.51220\n",
      "Training: loss = 1.24444, acc = 0.55769 | Val: loss = 1.21186, acc = 0.51220\n",
      "Training: loss = 1.30090, acc = 0.51573 | Val: loss = 1.21057, acc = 0.51220\n",
      "Training: loss = 1.24481, acc = 0.50175 | Val: loss = 1.20898, acc = 0.51220\n",
      "Training: loss = 1.22298, acc = 0.54196 | Val: loss = 1.20721, acc = 0.51220\n",
      "Training: loss = 1.21335, acc = 0.53671 | Val: loss = 1.20519, acc = 0.51220\n",
      "Training: loss = 1.30206, acc = 0.48776 | Val: loss = 1.20239, acc = 0.51220\n",
      "Training: loss = 1.14945, acc = 0.52622 | Val: loss = 1.19980, acc = 0.51220\n",
      "Training: loss = 1.18708, acc = 0.52273 | Val: loss = 1.19789, acc = 0.51220\n",
      "Training: loss = 1.21529, acc = 0.51923 | Val: loss = 1.19683, acc = 0.51220\n",
      "Training: loss = 1.19863, acc = 0.52098 | Val: loss = 1.19575, acc = 0.51220\n",
      "Training: loss = 1.16100, acc = 0.56469 | Val: loss = 1.19516, acc = 0.51220\n",
      "Training: loss = 1.23281, acc = 0.53846 | Val: loss = 1.19502, acc = 0.51220\n",
      "Training: loss = 1.19494, acc = 0.53497 | Val: loss = 1.19495, acc = 0.51220\n",
      "Training: loss = 1.25757, acc = 0.51224 | Val: loss = 1.19503, acc = 0.51220\n",
      "Training: loss = 1.21054, acc = 0.54196 | Val: loss = 1.19487, acc = 0.51220\n",
      "Training: loss = 1.24015, acc = 0.55070 | Val: loss = 1.19536, acc = 0.51220\n",
      "Training: loss = 1.21670, acc = 0.52622 | Val: loss = 1.19546, acc = 0.51220\n",
      "Training: loss = 1.17567, acc = 0.53322 | Val: loss = 1.19644, acc = 0.51220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: loss = 1.33896, acc = 0.52098 | Val: loss = 1.19852, acc = 0.51220\n",
      "Training: loss = 1.36983, acc = 0.51224 | Val: loss = 1.20132, acc = 0.51220\n",
      "Training: loss = 1.20374, acc = 0.54196 | Val: loss = 1.20477, acc = 0.51220\n",
      "Training: loss = 1.25355, acc = 0.51573 | Val: loss = 1.20743, acc = 0.51220\n",
      "Training: loss = 1.18218, acc = 0.56993 | Val: loss = 1.21043, acc = 0.51220\n",
      "Training: loss = 1.27286, acc = 0.51748 | Val: loss = 1.21326, acc = 0.50000\n",
      "Training: loss = 1.22384, acc = 0.55070 | Val: loss = 1.21666, acc = 0.50000\n",
      "Training: loss = 1.26885, acc = 0.47902 | Val: loss = 1.21917, acc = 0.50000\n",
      "Training: loss = 1.26200, acc = 0.50699 | Val: loss = 1.22154, acc = 0.50000\n",
      "Training: loss = 1.22632, acc = 0.54371 | Val: loss = 1.22414, acc = 0.50000\n",
      "Training: loss = 1.29936, acc = 0.50874 | Val: loss = 1.22525, acc = 0.50000\n",
      "Training: loss = 1.16298, acc = 0.54545 | Val: loss = 1.22556, acc = 0.50000\n",
      "Training: loss = 1.31371, acc = 0.51224 | Val: loss = 1.22453, acc = 0.50000\n",
      "Training: loss = 1.24088, acc = 0.51748 | Val: loss = 1.22320, acc = 0.50000\n",
      "Training: loss = 1.21818, acc = 0.53846 | Val: loss = 1.22181, acc = 0.50000\n",
      "Training: loss = 1.20739, acc = 0.52448 | Val: loss = 1.21994, acc = 0.50000\n",
      "Training: loss = 1.21425, acc = 0.53147 | Val: loss = 1.21695, acc = 0.50000\n",
      "Training: loss = 1.29737, acc = 0.48427 | Val: loss = 1.21392, acc = 0.50000\n",
      "Training: loss = 1.27870, acc = 0.50874 | Val: loss = 1.21184, acc = 0.50000\n",
      "Training: loss = 1.20467, acc = 0.56993 | Val: loss = 1.21005, acc = 0.50000\n",
      "Training: loss = 1.13789, acc = 0.55769 | Val: loss = 1.20825, acc = 0.50000\n",
      "Training: loss = 1.25355, acc = 0.51923 | Val: loss = 1.20633, acc = 0.50000\n",
      "Training: loss = 1.21477, acc = 0.53671 | Val: loss = 1.20496, acc = 0.50000\n",
      "Training: loss = 1.23410, acc = 0.54021 | Val: loss = 1.20394, acc = 0.51220\n",
      "Training: loss = 1.21168, acc = 0.54196 | Val: loss = 1.20240, acc = 0.51220\n",
      "Training: loss = 1.17051, acc = 0.54545 | Val: loss = 1.20177, acc = 0.51220\n",
      "Training: loss = 1.19520, acc = 0.57867 | Val: loss = 1.20123, acc = 0.51220\n",
      "Training: loss = 1.29798, acc = 0.52098 | Val: loss = 1.20173, acc = 0.51220\n",
      "Training: loss = 1.28685, acc = 0.51224 | Val: loss = 1.20128, acc = 0.51220\n",
      "Training: loss = 1.23168, acc = 0.52098 | Val: loss = 1.20217, acc = 0.51220\n",
      "Training: loss = 1.21145, acc = 0.53322 | Val: loss = 1.20421, acc = 0.51220\n",
      "Training: loss = 1.25208, acc = 0.54545 | Val: loss = 1.20652, acc = 0.51220\n",
      "Training: loss = 1.21988, acc = 0.55070 | Val: loss = 1.20870, acc = 0.51220\n",
      "Training: loss = 1.21110, acc = 0.58217 | Val: loss = 1.21084, acc = 0.51220\n",
      "Training: loss = 1.15456, acc = 0.53497 | Val: loss = 1.21282, acc = 0.51220\n",
      "Training: loss = 1.16861, acc = 0.56469 | Val: loss = 1.21541, acc = 0.51220\n",
      "Training: loss = 1.25424, acc = 0.51573 | Val: loss = 1.21745, acc = 0.51220\n",
      "Training: loss = 1.25220, acc = 0.53147 | Val: loss = 1.22059, acc = 0.51220\n",
      "Training: loss = 1.22071, acc = 0.53846 | Val: loss = 1.22300, acc = 0.51220\n",
      "Training: loss = 1.24865, acc = 0.54371 | Val: loss = 1.22501, acc = 0.51220\n",
      "Training: loss = 1.21854, acc = 0.50350 | Val: loss = 1.22617, acc = 0.51220\n",
      "Training: loss = 1.16799, acc = 0.58217 | Val: loss = 1.22789, acc = 0.51220\n",
      "Training: loss = 1.23340, acc = 0.56469 | Val: loss = 1.22947, acc = 0.51220\n",
      "Training: loss = 1.28132, acc = 0.54196 | Val: loss = 1.23080, acc = 0.51220\n",
      "Training: loss = 1.16716, acc = 0.56294 | Val: loss = 1.23231, acc = 0.50000\n",
      "Training: loss = 1.26838, acc = 0.52098 | Val: loss = 1.23413, acc = 0.50000\n",
      "Training: loss = 1.25429, acc = 0.52797 | Val: loss = 1.23553, acc = 0.50000\n",
      "Training: loss = 1.26867, acc = 0.53147 | Val: loss = 1.23664, acc = 0.50000\n",
      "Training: loss = 1.22649, acc = 0.51923 | Val: loss = 1.23696, acc = 0.50000\n",
      "Training: loss = 1.22136, acc = 0.52098 | Val: loss = 1.23666, acc = 0.50000\n",
      "Training: loss = 1.26286, acc = 0.51923 | Val: loss = 1.23536, acc = 0.50000\n",
      "Training: loss = 1.20268, acc = 0.53147 | Val: loss = 1.23319, acc = 0.50000\n",
      "Training: loss = 1.23936, acc = 0.51049 | Val: loss = 1.23167, acc = 0.50000\n",
      "Training: loss = 1.22545, acc = 0.52448 | Val: loss = 1.23000, acc = 0.50000\n",
      "Training: loss = 1.24207, acc = 0.52448 | Val: loss = 1.22713, acc = 0.50000\n",
      "Training: loss = 1.25112, acc = 0.48951 | Val: loss = 1.22432, acc = 0.50000\n",
      "Training: loss = 1.18259, acc = 0.54545 | Val: loss = 1.22305, acc = 0.50000\n",
      "Training: loss = 1.27532, acc = 0.51224 | Val: loss = 1.22272, acc = 0.50000\n",
      "Training: loss = 1.23122, acc = 0.55245 | Val: loss = 1.22269, acc = 0.50000\n",
      "Training: loss = 1.26954, acc = 0.54021 | Val: loss = 1.22251, acc = 0.51220\n",
      "Training: loss = 1.19193, acc = 0.54196 | Val: loss = 1.22208, acc = 0.51220\n",
      "Training: loss = 1.24223, acc = 0.51399 | Val: loss = 1.22234, acc = 0.51220\n",
      "Training: loss = 1.20883, acc = 0.52273 | Val: loss = 1.22358, acc = 0.51220\n",
      "Training: loss = 1.23580, acc = 0.55944 | Val: loss = 1.22446, acc = 0.51220\n",
      "Training: loss = 1.25220, acc = 0.51923 | Val: loss = 1.22557, acc = 0.51220\n",
      "Training: loss = 1.21716, acc = 0.52448 | Val: loss = 1.22735, acc = 0.51220\n",
      "Training: loss = 1.15640, acc = 0.54196 | Val: loss = 1.22950, acc = 0.51220\n",
      "Training: loss = 1.21051, acc = 0.52448 | Val: loss = 1.23183, acc = 0.51220\n",
      "Training: loss = 1.31100, acc = 0.52273 | Val: loss = 1.23423, acc = 0.51220\n",
      "Training: loss = 1.22488, acc = 0.52797 | Val: loss = 1.23605, acc = 0.51220\n",
      "Training: loss = 1.29780, acc = 0.51748 | Val: loss = 1.23786, acc = 0.51220\n",
      "Training: loss = 1.24079, acc = 0.50175 | Val: loss = 1.23939, acc = 0.51220\n",
      "Training: loss = 1.21334, acc = 0.53671 | Val: loss = 1.24021, acc = 0.50000\n",
      "Training: loss = 1.32661, acc = 0.51049 | Val: loss = 1.24103, acc = 0.50000\n",
      "Training: loss = 1.27015, acc = 0.51923 | Val: loss = 1.24334, acc = 0.50000\n",
      "Training: loss = 1.23237, acc = 0.52098 | Val: loss = 1.24510, acc = 0.50000\n",
      "Training: loss = 1.33366, acc = 0.48776 | Val: loss = 1.24759, acc = 0.50000\n",
      "Training: loss = 1.20715, acc = 0.51923 | Val: loss = 1.24788, acc = 0.50000\n",
      "Training: loss = 1.21340, acc = 0.53671 | Val: loss = 1.24733, acc = 0.50000\n",
      "Training: loss = 1.35066, acc = 0.49825 | Val: loss = 1.24627, acc = 0.50000\n",
      "Training: loss = 1.29468, acc = 0.46504 | Val: loss = 1.24460, acc = 0.50000\n",
      "Training: loss = 1.15897, acc = 0.54196 | Val: loss = 1.24219, acc = 0.50000\n",
      "Training: loss = 1.17208, acc = 0.54371 | Val: loss = 1.23918, acc = 0.50000\n",
      "Training: loss = 1.21428, acc = 0.53846 | Val: loss = 1.23630, acc = 0.50000\n",
      "Training: loss = 1.25176, acc = 0.51923 | Val: loss = 1.23345, acc = 0.50000\n",
      "Training: loss = 1.20768, acc = 0.53147 | Val: loss = 1.23179, acc = 0.50000\n",
      "Training: loss = 1.22766, acc = 0.53147 | Val: loss = 1.23034, acc = 0.50000\n",
      "Training: loss = 1.19488, acc = 0.55245 | Val: loss = 1.22947, acc = 0.50000\n",
      "Training: loss = 1.27107, acc = 0.50524 | Val: loss = 1.22881, acc = 0.50000\n",
      "Training: loss = 1.18793, acc = 0.54895 | Val: loss = 1.22853, acc = 0.51220\n",
      "Training: loss = 1.20915, acc = 0.54895 | Val: loss = 1.22769, acc = 0.51220\n",
      "Training: loss = 1.20658, acc = 0.53846 | Val: loss = 1.22715, acc = 0.51220\n",
      "Training: loss = 1.17774, acc = 0.55420 | Val: loss = 1.22715, acc = 0.51220\n",
      "Training: loss = 1.18491, acc = 0.57168 | Val: loss = 1.22727, acc = 0.51220\n",
      "Training: loss = 1.27567, acc = 0.50175 | Val: loss = 1.22721, acc = 0.51220\n",
      "Training: loss = 1.26467, acc = 0.54196 | Val: loss = 1.22788, acc = 0.50000\n",
      "Training: loss = 1.27627, acc = 0.53497 | Val: loss = 1.22840, acc = 0.50000\n",
      "Early stop! Min loss:  1.1948668956756592 , Max accuracy:  0.5365853905677795\n",
      "Early stop model validation loss:  1.4783241748809814 , accuracy:  0.5365853905677795\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ether_pre_trained.ckpt\n",
      "Test loss: 1.442426085472107 ; Test accuracy: 0.5864197611808777\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    with tf.name_scope('input'):\n",
    "        ftr_in = tf.placeholder(dtype=tf.float32, shape=(batch_size, nb_nodes, ft_size))\n",
    "        if sparse:\n",
    "            bias_in = tf.sparse_placeholder(dtype=tf.float32)\n",
    "        else:\n",
    "            bias_in = tf.placeholder(dtype=tf.float32, shape=(batch_size, nb_nodes, nb_nodes))\n",
    "        lbl_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes, nb_classes))\n",
    "        msk_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes))\n",
    "        attn_drop = tf.placeholder(dtype=tf.float32, shape=())\n",
    "        ffd_drop = tf.placeholder(dtype=tf.float32, shape=())\n",
    "        is_train = tf.placeholder(dtype=tf.bool, shape=())\n",
    "\n",
    "    logits = model.inference(ftr_in, nb_classes, nb_nodes, is_train,\n",
    "                                attn_drop, ffd_drop,\n",
    "                                bias_mat=bias_in,\n",
    "                                hid_units=hid_units, n_heads=n_heads,\n",
    "                                residual=residual, activation=nonlinearity)\n",
    "    log_resh = tf.reshape(logits, [-1, nb_classes])\n",
    "    lab_resh = tf.reshape(lbl_in, [-1, nb_classes])\n",
    "    msk_resh = tf.reshape(msk_in, [-1])\n",
    "    loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)\n",
    "    accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)\n",
    "\n",
    "    train_op = model.training(loss, lr, l2_coef)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "    vlss_mn = np.inf\n",
    "    vacc_mx = 0.0\n",
    "    curr_step = 0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "\n",
    "        train_loss_avg = 0\n",
    "        train_acc_avg = 0\n",
    "        val_loss_avg = 0\n",
    "        val_acc_avg = 0\n",
    "\n",
    "        for epoch in range(nb_epochs):\n",
    "            tr_step = 0\n",
    "            tr_size = features.shape[0]\n",
    "\n",
    "            while tr_step * batch_size < tr_size:\n",
    "                if sparse:\n",
    "                    bbias = biases\n",
    "                else:\n",
    "                    bbias = biases[tr_step*batch_size:(tr_step+1)*batch_size]\n",
    "\n",
    "                _, loss_value_tr, acc_tr = sess.run([train_op, loss, accuracy],\n",
    "                    feed_dict={\n",
    "                        ftr_in: features[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                        bias_in: bbias,\n",
    "                        lbl_in: y_train[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                        msk_in: train_mask[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                        is_train: True,\n",
    "                        attn_drop: 0.6, ffd_drop: 0.6})\n",
    "                train_loss_avg += loss_value_tr\n",
    "                train_acc_avg += acc_tr\n",
    "                tr_step += 1\n",
    "\n",
    "            vl_step = 0\n",
    "            vl_size = features.shape[0]\n",
    "\n",
    "            while vl_step * batch_size < vl_size:\n",
    "                if sparse:\n",
    "                    bbias = biases\n",
    "                else:\n",
    "                    bbias = biases[vl_step*batch_size:(vl_step+1)*batch_size]\n",
    "                loss_value_vl, acc_vl = sess.run([loss, accuracy],\n",
    "                    feed_dict={\n",
    "                        ftr_in: features[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                        bias_in: bbias,\n",
    "                        lbl_in: y_val[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                        msk_in: val_mask[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                        is_train: False,\n",
    "                        attn_drop: 0.0, ffd_drop: 0.0})\n",
    "                val_loss_avg += loss_value_vl\n",
    "                val_acc_avg += acc_vl\n",
    "                vl_step += 1\n",
    "\n",
    "            print('Training: loss = %.5f, acc = %.5f | Val: loss = %.5f, acc = %.5f' %\n",
    "                    (train_loss_avg/tr_step, train_acc_avg/tr_step,\n",
    "                    val_loss_avg/vl_step, val_acc_avg/vl_step))\n",
    "\n",
    "            if val_acc_avg/vl_step >= vacc_mx or val_loss_avg/vl_step <= vlss_mn:\n",
    "                if val_acc_avg/vl_step >= vacc_mx and val_loss_avg/vl_step <= vlss_mn:\n",
    "                    vacc_early_model = val_acc_avg/vl_step\n",
    "                    vlss_early_model = val_loss_avg/vl_step\n",
    "                    saver.save(sess, checkpt_file)\n",
    "                vacc_mx = np.max((val_acc_avg/vl_step, vacc_mx))\n",
    "                vlss_mn = np.min((val_loss_avg/vl_step, vlss_mn))\n",
    "                curr_step = 0\n",
    "            else:\n",
    "                curr_step += 1\n",
    "                if curr_step == patience:\n",
    "                    print('Early stop! Min loss: ', vlss_mn, ', Max accuracy: ', vacc_mx)\n",
    "                    print('Early stop model validation loss: ', vlss_early_model, ', accuracy: ', vacc_early_model)\n",
    "                    break\n",
    "\n",
    "            train_loss_avg = 0\n",
    "            train_acc_avg = 0\n",
    "            val_loss_avg = 0\n",
    "            val_acc_avg = 0\n",
    "\n",
    "        saver.restore(sess, checkpt_file)\n",
    "\n",
    "        ts_size = features.shape[0]\n",
    "        ts_step = 0\n",
    "        ts_loss = 0.0\n",
    "        ts_acc = 0.0\n",
    "\n",
    "        while ts_step * batch_size < ts_size:\n",
    "            if sparse:\n",
    "                bbias = biases\n",
    "            else:\n",
    "                bbias = biases[ts_step*batch_size:(ts_step+1)*batch_size]\n",
    "            loss_value_ts, acc_ts = sess.run([loss, accuracy],\n",
    "                feed_dict={\n",
    "                    ftr_in: features[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                    bias_in: bbias,\n",
    "                    lbl_in: y_test[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                    msk_in: test_mask[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                    is_train: False,\n",
    "                    attn_drop: 0.0, ffd_drop: 0.0})\n",
    "            ts_loss += loss_value_ts\n",
    "            ts_acc += acc_ts\n",
    "            ts_step += 1\n",
    "\n",
    "        print('Test loss:', ts_loss/ts_step, '; Test accuracy:', ts_acc/ts_step)\n",
    "\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
