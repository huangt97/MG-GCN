{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sparse\n",
    "from utils import *\n",
    "from model import *\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_sample_for_ether(walk_times,adj_sparse, train_index, batch_size, save_name, do_walk):\n",
    "    if do_walk:\n",
    "        nodes_num = len(train_index)\n",
    "        walks = torch.zeros(nodes_num, walk_times+1).long().cuda()\n",
    "        batches = create_batches_forList(train_index, batch_size, True)\n",
    "        i=0\n",
    "        degrees = torch.tensor(adj_sparse.sum(1)).view(-1).cuda()\n",
    "        candi_node = 0\n",
    "        for batch in batches:\n",
    "            walks[i*batch_size : i*batch_size + len(batch), 0] = torch.tensor(batch).cuda()\n",
    "            candi_node = sparse_mx_to_torch_sparse_tensor(adj_sparse[batch.cpu().numpy()]).cuda()\n",
    "            chosen_node = torch.zeros(len(batch), adj_sparse.shape[0]).cuda()\n",
    "            for id in range(len(batch)):\n",
    "                chosen_node[id][batch[id]] = 1.\n",
    "            candi_node = ((- chosen_node + candi_node)> 0.0).float()\n",
    "            for walk_id in range(walk_times):\n",
    "                for x in range(candi_node.shape[0]):\n",
    "                    if candi_node[x].sum()==0:\n",
    "                        candi_node[x][batch[x]] = 1.\n",
    "                p = candi_node * degrees\n",
    "                print(\"p = {}\".format(p))\n",
    "                new_node = torch.multinomial(p,1).squeeze(1)\n",
    "                walks[i*batch_size : i*batch_size + len(batch), walk_id+1] = new_node\n",
    "                for id in range(len(batch)):\n",
    "                    chosen_node[id][new_node[id]] = 1.\n",
    "                candi_node = candi_node - chosen_node + sparse_mx_to_torch_sparse_tensor(adj_sparse[new_node.cpu()]).cuda()\n",
    "                candi_node = (candi_node> 0.0).float()\n",
    "                \n",
    "            i+=1\n",
    "        torch.cuda.empty_cache()\n",
    "        result1 = np.array(walks.cpu())\n",
    "        io.savemat(save_name+'.mat',{save_name:result1})\n",
    "        return walks\n",
    "    else:\n",
    "        print(\"load path: {}\".format(save_name + '.mat'))\n",
    "        walks = io.loadmat(save_name+'.mat')\n",
    "        return torch.tensor(walks[save_name]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Model_Ether():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.model = AttnWeight(self.args).cuda()\n",
    "        self.sparse_adj, self.sparse_adj_train,self.sparse_adj_train_all, self.features, self.train_feature, self.train_feature_all,self.labels, self.train_labels, self.id_train, self.id_valid, self.id_test, num_labels = Origin_load_ether_data()\n",
    "        self.eval_labels = self.labels[self.id_valid]\n",
    "        self.test_labels = self.labels[self.id_test]\n",
    "        self.sample_batch_size = 256\n",
    "        print('train walk')\n",
    "        name = 'seed='+str(self.args.seed)+'_walk='+str(self.args.time)\n",
    "#         self.walks_train = pre_sample_for_ether(self.args.time, self.sparse_adj_train, self.id_train, self.sample_batch_size, args.walk_path + 'train_'+name,args.do_walk)\n",
    "#         print('valid walk')\n",
    "#         self.walks_valid = pre_sample_for_ether(self.args.time, self.sparse_adj, self.id_valid, self.sample_batch_size,args.walk_path + 'valid_'+name,args.do_walk)\n",
    "        self.sparse_adj.setdiag(1.0) \n",
    "        self.sparse_adj_train.setdiag(1.0) \n",
    "        self.nor_graph = normalize(self.sparse_adj, norm='l1', axis=1)\n",
    "        self.nor_graph_train = normalize(self.sparse_adj_train, norm='l1', axis=1)\n",
    "        self.nor_graph_train_all = normalize(self.sparse_adj_train_all, norm='l1', axis=1)\n",
    "        self.args.feature_dim = self.features.shape[1]\n",
    "        self.args.num_nodes = num_labels\n",
    "        self.args.num_labels = num_labels\n",
    "        print('cuda ready')\n",
    "        self.logs = create_logs(self.args)\n",
    "        self.best_loss = 1000\n",
    "        self.best_acc = 0\n",
    "        self.best_loss_both = 1000\n",
    "        self.best_acc_both = 0\n",
    "        self.stop_count = 0\n",
    "        self.best_loss_epoch = -1\n",
    "        self.best_both_epoch = -1\n",
    "        self.best_acc_epoch  = -1\n",
    "        self.epoch_idx = 0\n",
    "        self.total_time = 0.0\n",
    "\n",
    "    def fit(self):\n",
    "        print(\"\\nTraining started.\\n\")\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr = self.args.learning_rate, weight_decay = self.args.weight_decay)\n",
    "        self.optimizer.zero_grad()\n",
    "#         batches = create_batches_forWalk(self.walks_train, self.args.batch_size)\n",
    "#         valid_batch = create_batches_forWalk(self.walks_valid, self.args.batch_size)\n",
    "        self.id_train = list(self.id_train)\n",
    "        for epoch in tqdm(range(self.args.epochs)):\n",
    "            self.model.train()\n",
    "            # train\n",
    "            predictions = self.model(self.nor_graph_train_all, self.train_feature_all, self.args.time, self.sparse_adj_train, self.id_train, self.sample_batch_size)\n",
    "            prediction_loss = calculate_predictive_loss(self.train_labels, predictions)\n",
    "            acc, label_pre = calculate_reward(self.train_labels, predictions)\n",
    "            self.optimizer.zero_grad()\n",
    "            prediction_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            # eval\n",
    "            self.model.eval()\n",
    "            eval_predictions = self.model(self.nor_graph, self.features, self.args.time, self.sparse_adj, self.id_valid, self.sample_batch_size)\n",
    "            eval_prediction_loss = calculate_predictive_loss(self.eval_labels, eval_predictions)\n",
    "            eval_acc, eval_label_pre = calculate_reward(self.eval_labels, eval_predictions)\n",
    "            print(\"Train: loss = {} acc = {} | Valid: loss = {} acc = {}\".format(prediction_loss, acc, eval_prediction_loss, eval_acc))\n",
    "            \n",
    "#             for batch in valid_batch:\n",
    "#                 label = torch.index_select(self.labels, 0, batch[:,0].view(-1))\n",
    "#                 loss_node, acc, label_predict = self.process_node(label, batch, self.features, self.nor_graph)\n",
    "# #                 print(\"loss_node\",loss_node,\"acc:\",acc,\"label_predict:\",label_predict)\n",
    "#                 valid_loss += loss_node.item()\n",
    "#                 valid_acc += acc.item()\n",
    "                \n",
    "#             valid_loss = round(valid_loss*1000/len(self.id_valid), 4)\n",
    "#             valid_acc = round(valid_acc/len(self.id_valid), 4)\n",
    "#             self.acc_score = round(self.acc_score/len(self.id_train), 4)\n",
    "#             loss_score = round(self.epoch_loss*1000/len(self.id_train), 4)\n",
    "#             if epoch % 1 == 0:                 \n",
    "#                 print(\"epoch\",epoch,\"loss_train:\",prediction_loss,\"acc_train:\",\n",
    "#                                                      acc,'||',\"loss_valid:\",eval_prediction_loss,\"acc_valid:\",eval_acc)\n",
    "            if  eval_prediction_loss  < self.best_loss :\n",
    "                self.best_loss = eval_prediction_loss\n",
    "                torch.save(self.model.state_dict(),self.args.save_path_loss+\"best_model.pt\")\n",
    "                stop_count = 0\n",
    "                self.best_loss_epoch = epoch\n",
    "            else:\n",
    "                stop_count += 1\n",
    "                if stop_count == self.args.patience:\n",
    "                    print(self.args.patience, \"times no decrease\")\n",
    "                    return round(0/(epoch+1), 4), round(0 / (epoch+1), 4), time.time()\n",
    "        print('Max epoches reaches!')\n",
    "        return round(0/(epoch+1), 4), round(0 / (epoch+1), 4), time.time()\n",
    "            \n",
    "    def evaluation(self):\n",
    "        loss_result = torch.zeros(len(self.id_test), self.args.eva_times, dtype=torch.long)\n",
    "        acc_result = torch.zeros(len(self.id_test), self.args.eva_times, dtype=torch.long)\n",
    "        loss_acc = 0.0\n",
    "        acc_acc = 0.0\n",
    "        #print('test walk')\n",
    "        name = 'seed='+str(self.args.seed)+'_walk='+str(self.args.time)\n",
    "#         self.walks_test = pre_sample_for_ether(self.args.time, self.sparse_adj, self.id_test, self.sample_batch_size,self.args.walk_path + 'test_'+name,False)\n",
    "#         test_batch = create_batches_forWalk(self.walks_test, self.args.batch_size)\n",
    "\n",
    "        self.model.eval()\n",
    "        self.model.load_state_dict(torch.load(self.args.save_path_loss+\"best_model.pt\"))\n",
    "#         for batch in test_batch:\n",
    "#             label = torch.index_select(self.labels, 0, batch[:,0].view(-1))\n",
    "#             loss_node, acc, label_predict = self.process_node(label, batch, self.features, self.nor_graph)\n",
    "#             loss_acc += loss_node.item()\n",
    "#             acc_acc += acc.item()\n",
    "\n",
    "#         loss_acc = round(loss_acc*1000/len(self.id_test), 4)\n",
    "#         acc_acc = round(acc_acc/len(self.id_test), 4)\n",
    "        test_predictions = self.model(self.nor_graph, self.features, self.args.time, self.sparse_adj, self.id_test, self.sample_batch_size)\n",
    "        test_prediction_loss = calculate_predictive_loss(self.test_labels, test_predictions)\n",
    "        test_acc, test_label_pre = calculate_reward(self.test_labels, test_predictions)\n",
    "        test_prediction_loss, test_acc = test_prediction_loss.detach().cpu().numpy(), test_acc.detach().cpu().numpy()\n",
    "        print(\"loss acc:\", test_prediction_loss,\"acc_acc\",test_acc, 'load epoch:', self.best_loss_epoch)\n",
    "        return  test_prediction_loss, test_acc\n",
    "\n",
    "    def process_node(self,label, node, feature, graph):\n",
    "        prediction = self.model(node, graph, feature)\n",
    "        prediction_loss = calculate_predictive_loss(label, prediction)\n",
    "        acc, label_pre = calculate_reward(label, prediction)\n",
    "        return prediction_loss, acc, label_pre\n",
    "\n",
    "    def process_batch(self,label,batch, feature, graph):\n",
    "        self.optimizer.zero_grad()\n",
    "        pre = torch.cuda.memory_allocated()\n",
    "        batch_loss, acc, label_pre = self.process_node(label, batch, feature, graph)\n",
    "        self.acc_score += acc.item()\n",
    "        batch_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return batch_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class argument():\n",
    "    def __init__(self):\n",
    "        self.dataset = 'ether'\n",
    "        self.gamma = 1\n",
    "        self.hidden = 128\n",
    "        self.time = 3\n",
    "        self.save_path_acc = \"./best_ether_attn_feat_matrix/acc/\"\n",
    "        self.save_path_loss = \"./best_ether_attn_feat_matrix/loss/\"\n",
    "        self.save_path_both = \"./best_ether_attn_feat_matrix/both/\"\n",
    "        self.walk_path = \"./walks_ether_attention_feat_matrix/\"\n",
    "        self.patience = 30\n",
    "        self.batch_size = 256\n",
    "        self.epochs = 1000\n",
    "        self.learning_rate = 0.001\n",
    "        self.weight_decay = 0\n",
    "        self.num_labels = 41\n",
    "        self.feature_dim = 12\n",
    "        self.walk_times = 10\n",
    "        self.eva_times = 1\n",
    "        self.seed = 2019\n",
    "        self.dropout = 0\n",
    "        self.way = 'both'\n",
    "        self.conbime = 'Lstm'\n",
    "        self.do_walk = False\n",
    "args = argument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different walk time\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb4c25f03b3405dbaa54dee09fae6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different seed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59f0b69373b4c9c83975a6284d93e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features = (1402220, 12) type = <class 'numpy.ndarray'>\n",
      "train_index 572\n",
      "val_index 82\n",
      "test_index 162\n",
      "adj torch.Size([1402220, 1402220])\n",
      "features torch.Size([1402220, 12])\n",
      "train walk\n",
      "cuda ready\n",
      "\n",
      "Training started.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02dc84effe14c24a179dba64395dea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss = 7.715138912200928 acc = 0.008741258643567562 | Valid: loss = 7.637262344360352 acc = 0.03658536449074745\n",
      "Train: loss = 6.539260387420654 acc = 0.02622377499938011 | Valid: loss = 7.2444844245910645 acc = 0.060975607484579086\n",
      "Train: loss = 5.865132808685303 acc = 0.038461536169052124 | Valid: loss = 6.314367294311523 acc = 0.060975607484579086\n",
      "Train: loss = 5.189637184143066 acc = 0.050699301064014435 | Valid: loss = 5.968329906463623 acc = 0.0731707289814949\n",
      "Train: loss = 4.5876617431640625 acc = 0.07342657446861267 | Valid: loss = 5.502170562744141 acc = 0.0731707289814949\n",
      "Train: loss = 4.050677299499512 acc = 0.10839160531759262 | Valid: loss = 5.380706787109375 acc = 0.09756097197532654\n",
      "Train: loss = 3.7269556522369385 acc = 0.14335663616657257 | Valid: loss = 5.219001770019531 acc = 0.09756097197532654\n",
      "Train: loss = 3.434462785720825 acc = 0.21503496170043945 | Valid: loss = 4.581601142883301 acc = 0.08536584675312042\n",
      "Train: loss = 3.015364646911621 acc = 0.26923075318336487 | Valid: loss = 3.8522815704345703 acc = 0.15853658318519592\n",
      "Train: loss = 2.8245911598205566 acc = 0.2762237787246704 | Valid: loss = 3.9393773078918457 acc = 0.2073170691728592\n",
      "Train: loss = 2.6851348876953125 acc = 0.2937062978744507 | Valid: loss = 3.795473098754883 acc = 0.23170730471611023\n",
      "Train: loss = 2.366905689239502 acc = 0.3339160680770874 | Valid: loss = 3.28135347366333 acc = 0.24390242993831635\n",
      "Train: loss = 2.074943780899048 acc = 0.3898601233959198 | Valid: loss = 3.7273056507110596 acc = 0.2073170691728592\n",
      "Train: loss = 1.9215306043624878 acc = 0.4160839021205902 | Valid: loss = 3.5052900314331055 acc = 0.2926829159259796\n",
      "Train: loss = 1.7655370235443115 acc = 0.45279720425605774 | Valid: loss = 2.6340553760528564 acc = 0.32926827669143677\n",
      "Train: loss = 1.7134177684783936 acc = 0.46853145956993103 | Valid: loss = 3.213747262954712 acc = 0.2682926654815674\n"
     ]
    }
   ],
   "source": [
    "repeat_time = 10\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "ways = ['width']\n",
    "loss_loses = []\n",
    "loss_accs = []\n",
    "conbimes = ['Mean']\n",
    "print('Different walk time')\n",
    "#walk_timess = [0,4,5,6,7,8,9]\n",
    "for walk_time in tqdm(range(4,10)):\n",
    "    loss_accs = []\n",
    "    acc_accs = []\n",
    "    epoch_times = []\n",
    "    batch_times = []\n",
    "    total_times = []\n",
    "    print('Different seed')\n",
    "    for seed in tqdm(range(10,20)):\n",
    "        args = argument()\n",
    "        args.seed = seed\n",
    "        args.time = walk_time\n",
    "        torch.manual_seed(args.seed)\n",
    "        random.seed(args.seed)\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        np.random.seed(args.seed)\n",
    "        args.save_path_acc = \"./best_ether_attn_feat_matrix/acc/\"\n",
    "        args.save_path_loss = \"./best_ether_attn_feat_matrix/loss/\"\n",
    "        args.save_path_acc = args.save_path_acc+ 'ether' + '_dropout='+str(args.dropout)+'_wd='+str(args.weight_decay)+ '_b='+str(args.batch_size)+  '_lr='+str(args.learning_rate)+ '_epochs='+str(args.epochs)+'_walk='+ str(args.time)+'_'\n",
    "        args.save_path_loss = args.save_path_loss+ 'ether' + '_dropout='+str(args.dropout)+ '_wd='+str(args.weight_decay)+ '_b='+str(args.batch_size)+  '_lr='+str(args.learning_rate)+'_epochs='+str(args.epochs)+ '_walk='+ str(args.time)+'_'\n",
    "        model = Train_Model_Ether(args)\n",
    "        epoch_time, batch_time, total_time = model.fit()\n",
    "        loss_acc,acc_acc = model.evaluation()\n",
    "        epoch_times.append(epoch_time)\n",
    "        total_times.append(total_time)\n",
    "        batch_times.append(batch_time)\n",
    "        loss_accs.append(loss_acc)\n",
    "        acc_accs.append(acc_acc)\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    print('#####################################')\n",
    "    print(\"walk_time:\", args.time, 'ways:', args.way, 'conbimeï¼š', args.conbime)\n",
    "    print('batch_times:',sum(batch_times)/repeat_time)\n",
    "    print('epoch_times:',sum(epoch_times)/repeat_time)\n",
    "    print('total_times:',sum(total_times)/repeat_time)\n",
    "    print('loss_accs:',sum(loss_accs)/repeat_time,loss_accs)\n",
    "    print('acc_accs:',sum(acc_accs)/repeat_time,acc_accs)\n",
    "    print(sum(batch_times)/repeat_time, sum(epoch_times)/repeat_time, sum(loss_accs)/repeat_time, sum(acc_accs)/repeat_time)\n",
    "    print('#####################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
